{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to create a one dimentional convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    Implement a 1 dimentional convolution neural network classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_epoch : int\n",
    "        Number of epochs\n",
    "    \n",
    "    batch_size : int\n",
    "        Size of batch\n",
    "    \n",
    "    verbose : bool\n",
    "        True if outputting learning process\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : list\n",
    "        List of arrays of records of loss on train dataset\n",
    "    \n",
    "    val_loss : list\n",
    "        List of arrays of records of loss on validation dataset\n",
    "    \n",
    "    layers : list\n",
    "        List of layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_epoch=10, batch_size=10, verbose=True):\n",
    "        # Record hyperparameters as attribute\n",
    "        self.epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.layers = []\n",
    "    \n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers += [layer]\n",
    "    \n",
    "    \n",
    "    def forward_layer(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def backward_layer(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Correct values of train dataset\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation dataset\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples, )\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit\n",
    "        if self.verbose:\n",
    "            count = 0\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            # Initialize\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            \n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                get_mini_batch_val = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "                \n",
    "                for ((mini_X_train, mini_y_train), (mini_X_val_train, mini_y_val_train)) in zip(get_mini_batch, \n",
    "                                                                                                get_mini_batch_val):\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    Z3_val = self.forward_layer(mini_X_val_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                        L_val = loss.cross_entropy_loss(mini_y_val_train, Z3_val)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "                    dX_val = self.backward_layer(mini_y_val_train)\n",
    "            \n",
    "            else:\n",
    "                for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "            \n",
    "            # Output learning process if verbose is True\n",
    "            if self.verbose:\n",
    "                self.loss += [sum(L) / self.batch_size]\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    self.val_loss += [sum(L_val) / self.batch_size]\n",
    "                    print(\"{0}batch loss: {1}, val_loss: {2}\".format(count+1, self.loss[count], self.val_loss[count]))\n",
    "                else:\n",
    "                    print(self.loss[count])\n",
    "                count += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict by neural network classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples, 1)\n",
    "            Results of prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        Z3 = self.forward_layer(X)\n",
    "        \n",
    "        return np.argmax(Z3, axis=1)\n",
    "    \n",
    "    \n",
    "    def plot_learning_record(self):\n",
    "        \"\"\"\n",
    "        Plot learning records.\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(facecolor=\"azure\", edgecolor=\"coral\")\n",
    "        \n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        \n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def compute_index_values(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Index values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"accuracy score:\", accuracy_score(y, y_pred))\n",
    "    \n",
    "    \n",
    "    def plot_misclassification(self, X_val, y_val, y_pred):\n",
    "        \"\"\"\n",
    "        Plot results of misclassification. Show \"Results of prediction/Corrects\" above images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Results of prediction\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples,)\n",
    "            Correct labels of validation data\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Number of results I want to plot\n",
    "        num = 36\n",
    "\n",
    "        true_false = y_pred==y_val\n",
    "        false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "        if false_list.shape[0] < num:\n",
    "            num = false_list.shape[0]\n",
    "        \n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "        \n",
    "        for i in range(num):\n",
    "            ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "            ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "            ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer from a layer of n_nodes1 to a layer of n_nodes2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes of the previous layer\n",
    "    \n",
    "    n_nodes2 : int\n",
    "        Number of nodes of the following layer\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight\n",
    "    \n",
    "    B : ndarray, shape (n_nodes2,)\n",
    "        Bias\n",
    "    \n",
    "    Z : ndarray, shape (batch_size, n_nodes1)\n",
    "        Deepcopy of input\n",
    "    \n",
    "    dW : float\n",
    "        Gradient of weight\n",
    "    \n",
    "    dB : float\n",
    "        Gradient of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize self.W and self.B by using initializer method\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "        self.Z = 0\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.Z = copy.deepcopy(X)\n",
    "        \n",
    "        return np.dot(X, self.W) + self.B\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = np.average(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA) / dA.shape[0]\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization by Gaussian distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by Xavier's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by He's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(2/n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "        \n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update a weight and a bias of a layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    h : float\n",
    "        Sum of squares of all gradients up to the previous iterations about ith layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.h = 0\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update a weight and a bias of a layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.h += layer.dW * layer.dW\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(self.h+1e-7)\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    \"\"\"\n",
    "    Momentum SGD\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    momentum : float\n",
    "        Learning coefficient\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    vW : \n",
    "        Momentum term of weight\n",
    "    \n",
    "    vB : \n",
    "        Momentum term of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.vW = 0\n",
    "        self.vB = 0\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update a weight and a bias of a layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        layer : Instance of updated layer\n",
    "        \"\"\"\n",
    "\n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        self.vW = self.momentum*self.vW - self.lr*dW\n",
    "        self.vB = self.momentum*self.vB - self.lr*dB\n",
    "        \n",
    "        layer.W[...] = layer.W + self.vW\n",
    "        layer.B[...] = layer.B + self.vB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    beta1 : \n",
    "    \n",
    "    beta2 : \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    iter : int\n",
    "        Number of iterations\n",
    "    \n",
    "    mW : \n",
    "    \n",
    "    mB : \n",
    "    \n",
    "    vW : \n",
    "    \n",
    "    vB : \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        \n",
    "        self.iter = 0\n",
    "        self.mW = 0\n",
    "        self.mB = 0\n",
    "        self.vW = 0\n",
    "        self.vB = 0\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update a weight and a bias of a layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        layer : Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.iter += 1\n",
    "        \n",
    "        dW = np.dot(layer.Z.T, layer.dA) / len(layer.dA)\n",
    "        dB = np.mean(layer.dA, axis=0)\n",
    "        \n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter) \n",
    "        \n",
    "        self.mW += (1 - self.beta1) * (dW - self.mW)\n",
    "        self.vW += (1 - self.beta2) * (dW**2 - self.vW)\n",
    "        self.mB += (1 - self.beta1) * (dB - self.mB)\n",
    "        self.vB += (1 - self.beta2) * (dB**2 - self.vB)\n",
    "        \n",
    "        layer.W -= lr_t * self.mW / (np.sqrt(self.vW)+1e-7)\n",
    "        layer.B -= lr_t * self.mB / (np.sqrt(self.vB)+1e-7)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Z : ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        Z = 1 / (1+np.exp(-self.A))\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        d_sig = Z * (1-Z) * dA\n",
    "        \n",
    "        return d_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        Z = np.tanh(self.A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        d_tanh = (1 - Z**2)*dA\n",
    "        \n",
    "        return d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray, shape (batch_size, ith n_nodes)\n",
    "        Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        A -= np.max(A)\n",
    "        \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        \n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_samples, 1)\n",
    "            Correct values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \"\"\"\n",
    "        \n",
    "        d_soft = self.Z - y\n",
    "        \n",
    "        return d_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        Z = np.where(self.A<=0, 0, self.A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        d_relu = np.where(self.A<=0, 0, 1) * dA\n",
    "        \n",
    "        return d_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Compute loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def cross_entropy_loss(self, y, y_pred):\n",
    "            \"\"\"\n",
    "            Cross entropy error\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            y : ndarray, shape (n_samples, 1)\n",
    "                Correct values\n",
    "\n",
    "            y_pred : ndarray, shape (n_samples, 1)\n",
    "                Predicted values\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            ndarray, shape (n_samples, 1)\n",
    "                Cross entropy error\n",
    "            \"\"\"\n",
    "            \n",
    "            L = np.sum(-1 * y * np.log(y_pred+1e-10), axis=1)\n",
    "            \n",
    "            return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      Train dataset\n",
    "    \n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "      Correct values\n",
    "    \n",
    "    batch_size : int\n",
    "      Size of batch\n",
    "    \n",
    "    seed : int\n",
    "      Seed of random numbers of Numpy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dropout_ratio : float\n",
    "        Ratio of dropout\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    mask : float\n",
    "        Mask\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.mask = None\n",
    "    \n",
    "    \n",
    "    def forward(self, X, train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_ratio\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1-self.dropout_ratio)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return dA * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Create a Class of One Deimentional Convolution Layer the Number of Channels is 1\n",
    "\n",
    "<br />\n",
    "\n",
    "I do not think of any paddings, and I let the number of strides 1. Also, this class does not handle some data at the same time and apply to only sigle batch size, 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forwardpropagation\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : $i$th value of outputted array\n",
    "\n",
    "$F$ : Size of filter\n",
    "\n",
    "$x_{(i+s)}$ : $(i+s)$th value of inputted array\n",
    "\n",
    "$w_s$ : $s$th value of array of weight\n",
    "\n",
    "$b$ : Bias term\n",
    "\n",
    "<br />\n",
    "\n",
    "All of them are scalers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Equation\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : Learning rate\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : Gradient of loss $L$ for $w_s$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : Gradient of loss $L$ for $b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : $i$th value of array of gradient\n",
    "\n",
    "$N_{out}$ : Size of output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Given to Forward Layer\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : $j$th value of array of error transferred to the forward layer\n",
    "\n",
    "<br />\n",
    "\n",
    "When $j - s < 0$ or $j - s > N_{out} - 1$, $\\frac{\\partial L}{\\partial a_{(j-s)}}$ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Compute Size of Outputs After One Dimentional Convolution\n",
    "\n",
    "<br />\n",
    "\n",
    "The following equation shows how the number of features change by convolution. It includes padding and stride.\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\n",
    "$$\n",
    "\n",
    "$N_{out}$ : Size of outputs (Number of features)\n",
    "\n",
    "$N_{in}$ : Size of inputs (Number of features)\n",
    "\n",
    "$P$ : Number of paddings to a certain direction\n",
    "\n",
    "$F$ : Size of filter\n",
    "\n",
    "$S$ : Size of stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    One dimentional convolution layer where the number of channels is 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    filter_size : int\n",
    "        Size of a filter\n",
    "    \n",
    "    straid : int\n",
    "        Size of a straid\n",
    "    \n",
    "    pad : int\n",
    "        Size of a padding\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight\n",
    "    \n",
    "    B : ndarray, shape (n_nodes2,)\n",
    "        Bias\n",
    "    \n",
    "    out_size : int\n",
    "        Size of outputs\n",
    "    \n",
    "    dW : float\n",
    "        Gradient of weight\n",
    "    \n",
    "    dB : float\n",
    "        Gradient of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initializer, optimizer, filter_size, straid=1, padding=0):\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.filter_size = filter_size\n",
    "        self.straid = straid\n",
    "        self.pad = padding\n",
    "        \n",
    "        # Initialize self.W and self.B by using initializer method\n",
    "        self.W = self.initializer.W(self.filter_size, 1)\n",
    "        self.B = self.initializer.B(self.filter_size, 1)\n",
    "        \n",
    "        self.out_size = None\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.out_size = int((len(X) - self.filter_size)/self.straid + 1)\n",
    "        \n",
    "        A = np.zeros(self.out_size)\n",
    "        for i in range(self.out_size):\n",
    "            A[i] = np.dot(X[i:i+self.filter_size], self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        \n",
    "        for i in range(self.out_size):\n",
    "            self.dW[i] = np.dot(dA[i], X[i:i+self.filter_size])\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "        \"\"\"\n",
    "        Image to Column\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_data : ndarray, shape (Number of data, Number of channels, Height, Width)\n",
    "            4 dimentional input data\n",
    "\n",
    "        filter_h : int\n",
    "            Height of a filter\n",
    "\n",
    "        filter_w : int\n",
    "            Width of a filter\n",
    "\n",
    "        stride : int\n",
    "            Size of a stride\n",
    "\n",
    "        pad : int\n",
    "            Size of a padding\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        col : ndarray, shape (Height, Width)\n",
    "            2 dimentional output as column\n",
    "        \"\"\"\n",
    "\n",
    "        N, C, H, W = input_data.shape\n",
    "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "        for y in range(filter_h):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(filter_w):\n",
    "                x_max = x + stride*out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "\n",
    "        return col\n",
    "    \n",
    "    \n",
    "    def compute_size_of_output(self, n_in):\n",
    "        n_out = (n_in + 2*self.pad - self.filter_size)/self.straid + 1\n",
    "        return n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Validate a One Dimentional Convolution Layer by Using a Simple Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "# Input\n",
    "x = np.array([1,2,3,4])\n",
    "\n",
    "# Weight\n",
    "w = np.array([3, 5, 7])\n",
    "\n",
    "# Bias\n",
    "b = np.array([1])\n",
    "\n",
    "# Size of a filter\n",
    "filter_size = 3\n",
    "\n",
    "# Size of a straid\n",
    "straid = 1\n",
    "\n",
    "# Loss\n",
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwardpropagation\n",
    "\n",
    "out_size = int((len(x) - filter_size)/straid + 1)\n",
    "\n",
    "a = np.zeros(out_size)\n",
    "for i in range(out_size):\n",
    "    a[i] = np.dot(x[i:i+filter_size], w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(3,)\n",
      "(2,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c9130fc0700f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdelta_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdeltaz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "\n",
    "delta_b = np.sum(delta_a, axis=0)\n",
    "\n",
    "delta_w = np.zeros(out_size)\n",
    "for i in range(out_size):\n",
    "    print(delta_w.shape)\n",
    "    print(x[i:i+filter_size].shape)\n",
    "    print(delta_a.shape)\n",
    "    delta_w[i] = np.dot(x[i:i+filter_size].T, delta_a[i])\n",
    "\n",
    "deltaz = np.dot(delta_a, w.T)\n",
    "\n",
    "# Update\n",
    "#self = self.optimizer.update(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[0. 0.]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'delta_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f5b2c29efe04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'delta_x' is not defined"
     ]
    }
   ],
   "source": [
    "print(delta_b)\n",
    "print(delta_w)\n",
    "print(delta_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
