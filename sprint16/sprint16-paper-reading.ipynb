{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Reading\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to read a paper, \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\", then cite it and summarize the main contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) What methods did exist in the field of object detection?\n",
    "\n",
    "- Abstract, 1 INTRODUCTION\n",
    "\n",
    "    \n",
    "    - R-CNN\n",
    "    \n",
    "    - Fast R-CNN\n",
    "    \n",
    "    - SPPnet\n",
    "    \n",
    "    - An object detection system based on mixtures of multiscale deformable part models provided by the previous research, \"Object Detection with Discriminatively Trained Part Based Models\"\n",
    "    \n",
    "    - An integrated framework for using Convolutional Networks for classification, localization and detection provided by the previous research, \"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) How could the authors make the Fast R-CNN faster?\n",
    "\n",
    "- Abstract\n",
    "\n",
    "\n",
    "    - A Region Proposal Network (RPN) shares full-image convolutional features with the detection network, thus it enables nearly cost-free region proposals.\n",
    "    \n",
    "    - A RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.\n",
    "    \n",
    "    - The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.\n",
    "    \n",
    "    - The authors further merge RPN and Fast R-CNN into a single network by sharing their convolutional features—using the recently popular terminology of neural networks with “attention” mechanisms, the RPN component tells the unified network where to look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) What are differences between One-Stage Detection and Two-Stage Proposal + Detection?\n",
    "\n",
    "- One-Stage Detection vs. Two-Stage Proposal + De- tection.\n",
    "\n",
    "\n",
    "    - The OverFeat paper, \"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\", proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps.\n",
    "    \n",
    "    - OverFeat is a one-stage, class-specific detection pipeline, and Faster R-CNN is a two-stage cascade consisting of class-agnostic proposals and class-specific detections.\n",
    "    \n",
    "    - In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3×3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios.\n",
    "    \n",
    "    - Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN—the downstream Fast R-CNN detector attends to the proposals to refine them.\n",
    "    \n",
    "    - In the second stage of Faster R-CNN's cascade, the region-wise features are adaptively pooled from proposal boxes that more faithfully cover the features of the regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) What is a RPN?\n",
    "\n",
    "- 3.1 Region Proposal Networks\n",
    "\n",
    "\n",
    "    - The RPN stands for Region Proposal Network that takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) What is a RoI pooling?\n",
    "\n",
    "- 2.1. The RoI pooling layer of a paper, \"Fast R-CNN\"\n",
    "\n",
    "\n",
    "    - The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI.\n",
    "\n",
    "    - RoI max pooling works by dividing the h × w RoI window into an H × W grid of sub-windows of approximate size h/H × w/W and then max-pooling the values in each sub-window into the corresponding output grid cell.\n",
    "    \n",
    "    - Pooling is applied independently to each feature map channel, as in standard max pooling.\n",
    "    \n",
    "    - The RoI layer is simply the special-case of the spatial pyramid pooling layer used in SPPnets provided by a paper, \"Spatial pyramid pooling in deep convolutional networks for visual recognition\", in which there is only one pyramid level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) What size of anchors is the best?\n",
    "\n",
    "- 3.1.1 Anchors\n",
    "\n",
    "\n",
    "    - An anchor is centered at the sliding window in question, and is associated with a scale and aspect ratio. By default the authors use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position.\n",
    "\n",
    "<br />\n",
    "\n",
    "- Multi-Scale Anchors as Regression References\n",
    "\n",
    "\n",
    "    - An anchor-based method provided by the authors is built on a pyramid of anchors.\n",
    "    \n",
    "    - The method classifies and regresses bounding boxes with reference to anchor boxes of multiple scales and aspect ratios.\n",
    "    \n",
    "    - It only relies on images and feature maps of a single scale, and uses filters (sliding windows on the feature map) of a single size.\n",
    "\n",
    "    - Because of this multi-scale design based on anchors, we can simply use the convolutional features computed on a single-scale image, as is also done by the Fast R-CNN detector provided by a paper, \"Fast R-CNN\". The design of multi-scale anchors is a key component for sharing features without extra cost for addressing scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) What datasets were used? Compared to previous research, what index values are used?\n",
    "\n",
    "- 1 INTRODUCTION, 4.1 Experiments on PASCAL VOC, 4.2 Experiments on MS COCO, 4.3 From MS COCO to PASCAL VOC\n",
    "\n",
    "\n",
    "    - The used datasets are the PASCAL VOC detection benchmarks and the Microsoft COCO object detection dataset.\n",
    "\n",
    "<br />\n",
    "\n",
    "- 4.1 Experiments on PASCAL VOC\n",
    "\n",
    "\n",
    "    - The used index value is mean Average Precision (mAP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) How did the Faster R-CNN cited on a newer paper of object detection?\n",
    "\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
