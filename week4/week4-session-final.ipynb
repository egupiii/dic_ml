{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Final\n",
    "\n",
    "\n",
    "# Fit and Predict Credit Information\n",
    "\n",
    "\n",
    "I am going to fit and predit credit information of a competition, \"Home Credit Default Risk\" on Kaggle.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/home-credit-default-risk\n",
    "\n",
    "\n",
    "## [Task 1] Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kazukiegusa/.pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kazukiegusa/.pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# get the train and test dataset\n",
    "train = pd.read_csv('\"Home Credit Default Risk\"_application_train.csv')\n",
    "test = pd.read_csv('\"Home Credit Default Risk\"_application_test.csv')\n",
    "\n",
    "# concatenate the train dataset with the test dataset\n",
    "train_test = train.append(test)\n",
    "\n",
    "# create dummies\n",
    "new_train_test = pd.get_dummies(train_test)\n",
    "\n",
    "#  compute the correlation coefficients\n",
    "corr_mat = new_train_test.corr()\n",
    "\n",
    "# split the concatenated dataset into the train dataset and the test dataset\n",
    "train_input = new_train_test[0: train.shape[0]].copy()\n",
    "test_input = new_train_test[train.shape[0]:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.262326\n",
      "[2]\tvalid_0's binary_logloss: 0.25312\n",
      "[3]\tvalid_0's binary_logloss: 0.251118\n",
      "[4]\tvalid_0's binary_logloss: 0.250335\n",
      "[5]\tvalid_0's binary_logloss: 0.250268\n",
      "[6]\tvalid_0's binary_logloss: 0.250072\n",
      "[7]\tvalid_0's binary_logloss: 0.249893\n",
      "[8]\tvalid_0's binary_logloss: 0.249758\n",
      "[9]\tvalid_0's binary_logloss: 0.249673\n",
      "[10]\tvalid_0's binary_logloss: 0.249562\n",
      "[11]\tvalid_0's binary_logloss: 0.249642\n",
      "[12]\tvalid_0's binary_logloss: 0.24955\n",
      "[13]\tvalid_0's binary_logloss: 0.249641\n",
      "[14]\tvalid_0's binary_logloss: 0.249725\n",
      "[15]\tvalid_0's binary_logloss: 0.249621\n",
      "[16]\tvalid_0's binary_logloss: 0.249718\n",
      "[17]\tvalid_0's binary_logloss: 0.249586\n",
      "[18]\tvalid_0's binary_logloss: 0.24948\n",
      "[19]\tvalid_0's binary_logloss: 0.249417\n",
      "[20]\tvalid_0's binary_logloss: 0.249535\n",
      "[21]\tvalid_0's binary_logloss: 0.249576\n",
      "[22]\tvalid_0's binary_logloss: 0.249498\n",
      "[23]\tvalid_0's binary_logloss: 0.249422\n",
      "[24]\tvalid_0's binary_logloss: 0.24936\n",
      "[25]\tvalid_0's binary_logloss: 0.24932\n",
      "[26]\tvalid_0's binary_logloss: 0.249319\n",
      "[27]\tvalid_0's binary_logloss: 0.249288\n",
      "[28]\tvalid_0's binary_logloss: 0.249263\n",
      "[29]\tvalid_0's binary_logloss: 0.249282\n",
      "[30]\tvalid_0's binary_logloss: 0.249231\n",
      "[31]\tvalid_0's binary_logloss: 0.249202\n",
      "[32]\tvalid_0's binary_logloss: 0.249196\n",
      "[33]\tvalid_0's binary_logloss: 0.249216\n",
      "[34]\tvalid_0's binary_logloss: 0.249269\n",
      "[35]\tvalid_0's binary_logloss: 0.249234\n",
      "[36]\tvalid_0's binary_logloss: 0.249213\n",
      "[37]\tvalid_0's binary_logloss: 0.249212\n",
      "[38]\tvalid_0's binary_logloss: 0.249231\n",
      "[39]\tvalid_0's binary_logloss: 0.249292\n",
      "[40]\tvalid_0's binary_logloss: 0.249303\n",
      "[41]\tvalid_0's binary_logloss: 0.249375\n",
      "[42]\tvalid_0's binary_logloss: 0.249339\n",
      "[43]\tvalid_0's binary_logloss: 0.24937\n",
      "[44]\tvalid_0's binary_logloss: 0.249335\n",
      "[45]\tvalid_0's binary_logloss: 0.249349\n",
      "[46]\tvalid_0's binary_logloss: 0.249399\n",
      "[47]\tvalid_0's binary_logloss: 0.24937\n",
      "[48]\tvalid_0's binary_logloss: 0.249388\n",
      "[49]\tvalid_0's binary_logloss: 0.249437\n",
      "[50]\tvalid_0's binary_logloss: 0.249451\n",
      "[51]\tvalid_0's binary_logloss: 0.249418\n",
      "[52]\tvalid_0's binary_logloss: 0.24943\n",
      "[53]\tvalid_0's binary_logloss: 0.249445\n",
      "[54]\tvalid_0's binary_logloss: 0.249407\n",
      "[55]\tvalid_0's binary_logloss: 0.249377\n",
      "[56]\tvalid_0's binary_logloss: 0.249342\n",
      "[57]\tvalid_0's binary_logloss: 0.249319\n",
      "[58]\tvalid_0's binary_logloss: 0.249371\n",
      "[59]\tvalid_0's binary_logloss: 0.249383\n",
      "[60]\tvalid_0's binary_logloss: 0.249353\n",
      "[61]\tvalid_0's binary_logloss: 0.249364\n",
      "[62]\tvalid_0's binary_logloss: 0.249343\n",
      "[63]\tvalid_0's binary_logloss: 0.249357\n",
      "[64]\tvalid_0's binary_logloss: 0.249336\n",
      "[65]\tvalid_0's binary_logloss: 0.249356\n",
      "[66]\tvalid_0's binary_logloss: 0.249327\n",
      "[67]\tvalid_0's binary_logloss: 0.249315\n",
      "[68]\tvalid_0's binary_logloss: 0.249296\n",
      "[69]\tvalid_0's binary_logloss: 0.249269\n",
      "[70]\tvalid_0's binary_logloss: 0.249248\n",
      "[71]\tvalid_0's binary_logloss: 0.249292\n",
      "[72]\tvalid_0's binary_logloss: 0.24927\n",
      "[73]\tvalid_0's binary_logloss: 0.249238\n",
      "[74]\tvalid_0's binary_logloss: 0.249223\n",
      "[75]\tvalid_0's binary_logloss: 0.249205\n",
      "[76]\tvalid_0's binary_logloss: 0.249202\n",
      "[77]\tvalid_0's binary_logloss: 0.249188\n",
      "[78]\tvalid_0's binary_logloss: 0.249212\n",
      "[79]\tvalid_0's binary_logloss: 0.249201\n",
      "[80]\tvalid_0's binary_logloss: 0.249213\n",
      "[81]\tvalid_0's binary_logloss: 0.249197\n",
      "[82]\tvalid_0's binary_logloss: 0.249183\n",
      "[83]\tvalid_0's binary_logloss: 0.249188\n",
      "[84]\tvalid_0's binary_logloss: 0.249178\n",
      "[85]\tvalid_0's binary_logloss: 0.249179\n",
      "[86]\tvalid_0's binary_logloss: 0.249167\n",
      "[87]\tvalid_0's binary_logloss: 0.249159\n",
      "[88]\tvalid_0's binary_logloss: 0.249147\n",
      "[89]\tvalid_0's binary_logloss: 0.249133\n",
      "[90]\tvalid_0's binary_logloss: 0.249124\n",
      "[91]\tvalid_0's binary_logloss: 0.249113\n",
      "[92]\tvalid_0's binary_logloss: 0.249136\n",
      "[93]\tvalid_0's binary_logloss: 0.249128\n",
      "[94]\tvalid_0's binary_logloss: 0.249132\n",
      "[95]\tvalid_0's binary_logloss: 0.249125\n",
      "[96]\tvalid_0's binary_logloss: 0.249166\n",
      "[97]\tvalid_0's binary_logloss: 0.249154\n",
      "[98]\tvalid_0's binary_logloss: 0.249157\n",
      "[99]\tvalid_0's binary_logloss: 0.249144\n",
      "[100]\tvalid_0's binary_logloss: 0.249152\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "X = train_input.drop(\"TARGET\",axis=1).values\n",
    "y = train_input.loc[:,'TARGET'].values\n",
    "\n",
    "lgbm_train = lgbm.Dataset(X,y)\n",
    "\n",
    "lgbm_eval = lgbm.Dataset(X, y, reference=lgbm_train)\n",
    "\n",
    "lgbm_params = {'objective':'binary',\n",
    "               'boosting_type': 'rf',\n",
    "               'subsample': 0.623,\n",
    "               'colsample_bytree': 0.7,\n",
    "               'num_leaves': 127,\n",
    "               'max_depth': 8,\n",
    "               'seed': 99,\n",
    "               'bagging_freq': 1,\n",
    "               'n_jobs': 4}\n",
    "\n",
    "model = lgbm.train(lgbm_params, lgbm_train, valid_sets=lgbm_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7570752634565502"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the test dataset\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# compute AUC\n",
    "roc_auc_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1) ORGANIZATION_TYPE_Insurance    0.000000\n",
      " 2) ORGANIZATION_TYPE_Construction 0.000000\n",
      " 3) ORGANIZATION_TYPE_Electricity  0.000000\n",
      " 4) ORGANIZATION_TYPE_Emergency    0.000000\n",
      " 5) ORGANIZATION_TYPE_Government   0.000000\n",
      " 6) ORGANIZATION_TYPE_Housing      0.000000\n",
      " 7) ORGANIZATION_TYPE_Industry: type 1 0.000000\n",
      " 8) ORGANIZATION_TYPE_Industry: type 10 0.000000\n",
      " 9) ORGANIZATION_TYPE_Industry: type 11 0.000000\n",
      "10) ORGANIZATION_TYPE_Industry: type 13 0.000000\n",
      "11) ORGANIZATION_TYPE_Industry: type 2 0.000000\n",
      "12) ORGANIZATION_TYPE_Industry: type 3 0.000000\n",
      "13) ORGANIZATION_TYPE_Industry: type 5 0.000000\n",
      "14) ORGANIZATION_TYPE_Industry: type 6 0.000000\n",
      "15) ORGANIZATION_TYPE_Industry: type 7 0.000000\n",
      "16) ORGANIZATION_TYPE_Industry: type 8 0.000000\n",
      "17) ORGANIZATION_TYPE_Industry: type 9 0.000000\n",
      "18) ORGANIZATION_TYPE_Kindergarten 0.000000\n",
      "19) ORGANIZATION_TYPE_Medicine     0.000000\n",
      "20) ORGANIZATION_TYPE_Military     0.000000\n",
      "21) ORGANIZATION_TYPE_Mobile       0.000000\n",
      "22) ORGANIZATION_TYPE_Other        0.000000\n",
      "23) ORGANIZATION_TYPE_Postal       0.000000\n",
      "24) FLAG_MOBIL                     0.000000\n",
      "25) ORGANIZATION_TYPE_Business Entity Type 3 0.000000\n",
      "26) FLAG_DOCUMENT_9                0.000000\n",
      "27) ORGANIZATION_TYPE_Business Entity Type 1 0.000000\n",
      "28) ORGANIZATION_TYPE_Advertising  0.000000\n",
      "29) NAME_TYPE_SUITE_Family         0.000000\n",
      "30) NAME_TYPE_SUITE_Other_B        0.000000\n",
      "31) NAME_INCOME_TYPE_Working       0.000000\n",
      "32) NAME_INCOME_TYPE_Unemployed    0.000000\n",
      "33) NAME_TYPE_SUITE_Spouse, partner 0.000000\n",
      "34) NAME_INCOME_TYPE_Pensioner     0.000000\n",
      "35) NAME_INCOME_TYPE_Commercial associate 0.000000\n",
      "36) NAME_HOUSING_TYPE_Rented apartment 0.000000\n",
      "37) NAME_HOUSING_TYPE_House / apartment 0.000000\n",
      "38) NAME_FAMILY_STATUS_Widow       0.000000\n",
      "39) OCCUPATION_TYPE_High skill tech staff 0.000000\n",
      "40) OCCUPATION_TYPE_Laborers       0.000000\n",
      "41) NAME_EDUCATION_TYPE_Higher education 0.000000\n",
      "42) OCCUPATION_TYPE_Private service staff 0.000000\n",
      "43) NAME_CONTRACT_TYPE_Cash loans  0.000000\n",
      "44) HOUSETYPE_MODE_specific housing 0.000000\n",
      "45) FONDKAPREMONT_MODE_reg oper account 0.000000\n",
      "46) OCCUPATION_TYPE_Realty agents  0.000000\n",
      "47) OCCUPATION_TYPE_Sales staff    0.000000\n",
      "48) FLAG_OWN_CAR_N                 0.000000\n",
      "49) EMERGENCYSTATE_MODE_Yes        0.000000\n",
      "50) EMERGENCYSTATE_MODE_No         0.000000\n",
      "51) OCCUPATION_TYPE_Security staff 0.000000\n",
      "52) ORGANIZATION_TYPE_Agriculture  0.000000\n",
      "53) FLAG_DOCUMENT_7                0.000000\n",
      "54) NAME_TYPE_SUITE_Other_A        0.000000\n",
      "55) FLAG_DOCUMENT_17               0.000000\n",
      "56) FLAG_DOCUMENT_10               0.000000\n",
      "57) WEEKDAY_APPR_PROCESS_START_FRIDAY 0.000000\n",
      "58) FLAG_DOCUMENT_11               0.000000\n",
      "59) FLAG_DOCUMENT_12               0.000000\n",
      "60) FLAG_DOCUMENT_13               0.000000\n",
      "61) FLAG_DOCUMENT_14               0.000000\n",
      "62) FLAG_DOCUMENT_15               0.000000\n",
      "63) ORGANIZATION_TYPE_Self-employed 0.000000\n",
      "64) ORGANIZATION_TYPE_Telecom      0.000000\n",
      "65) ORGANIZATION_TYPE_Trade: type 1 0.000000\n",
      "66) ORGANIZATION_TYPE_Trade: type 2 0.000000\n",
      "67) FLAG_CONT_MOBILE               0.000000\n",
      "68) FLAG_DOCUMENT_18               0.000000\n",
      "69) ORGANIZATION_TYPE_Restaurant   0.000000\n",
      "70) ORGANIZATION_TYPE_Trade: type 5 0.000000\n",
      "71) FLAG_DOCUMENT_4                0.000000\n",
      "72) ORGANIZATION_TYPE_XNA          0.000000\n",
      "73) WALLSMATERIAL_MODE_Others      0.000000\n",
      "74) ORGANIZATION_TYPE_Transport: type 3 0.000000\n",
      "75) ORGANIZATION_TYPE_Trade: type 3 0.000000\n",
      "76) ORGANIZATION_TYPE_Transport: type 2 0.000000\n",
      "77) FLAG_DOCUMENT_21               0.000000\n",
      "78) FLAG_DOCUMENT_20               0.000000\n",
      "79) ORGANIZATION_TYPE_Trade: type 7 0.000000\n",
      "80) FLAG_DOCUMENT_2                0.000000\n",
      "81) ORGANIZATION_TYPE_Trade: type 6 0.000000\n",
      "82) FLAG_DOCUMENT_19               0.000000\n",
      "83) HOUSETYPE_MODE_block of flats  1.000000\n",
      "84) ORGANIZATION_TYPE_School       1.000000\n",
      "85) HOUSETYPE_MODE_terraced house  1.000000\n",
      "86) ORGANIZATION_TYPE_Security     1.000000\n",
      "87) ORGANIZATION_TYPE_Bank         1.000000\n",
      "88) ORGANIZATION_TYPE_Industry: type 12 1.000000\n",
      "89) ORGANIZATION_TYPE_Realtor      1.000000\n",
      "90) FLAG_DOCUMENT_16               1.000000\n",
      "91) ORGANIZATION_TYPE_Legal Services 1.000000\n",
      "92) ORGANIZATION_TYPE_Industry: type 4 2.000000\n",
      "93) ORGANIZATION_TYPE_Religion     2.000000\n",
      "94) WALLSMATERIAL_MODE_Mixed       2.000000\n",
      "95) FONDKAPREMONT_MODE_not specified 2.000000\n",
      "96) NAME_INCOME_TYPE_Student       2.000000\n",
      "97) ORGANIZATION_TYPE_Police       2.000000\n",
      "98) AMT_REQ_CREDIT_BUREAU_HOUR     2.000000\n",
      "99) NAME_HOUSING_TYPE_Co-op apartment 2.000000\n",
      "100) ORGANIZATION_TYPE_Business Entity Type 2 2.000000\n",
      "101) NAME_TYPE_SUITE_Unaccompanied  2.000000\n",
      "102) ORGANIZATION_TYPE_Hotel        2.000000\n",
      "103) WALLSMATERIAL_MODE_Stone, brick 2.000000\n",
      "104) LIVE_REGION_NOT_WORK_REGION    2.000000\n",
      "105) OCCUPATION_TYPE_Cleaning staff 2.000000\n",
      "106) REG_REGION_NOT_LIVE_REGION     2.000000\n",
      "107) WALLSMATERIAL_MODE_Block       2.000000\n",
      "108) REG_REGION_NOT_WORK_REGION     2.000000\n",
      "109) OCCUPATION_TYPE_IT staff       3.000000\n",
      "110) WALLSMATERIAL_MODE_Monolithic  3.000000\n",
      "111) ORGANIZATION_TYPE_Trade: type 4 3.000000\n",
      "112) ORGANIZATION_TYPE_University   3.000000\n",
      "113) WALLSMATERIAL_MODE_Panel       3.000000\n",
      "114) NAME_HOUSING_TYPE_With parents 3.000000\n",
      "115) LIVE_CITY_NOT_WORK_CITY        4.000000\n",
      "116) ELEVATORS_MEDI                 4.000000\n",
      "117) OCCUPATION_TYPE_Medicine staff 4.000000\n",
      "118) WEEKDAY_APPR_PROCESS_START_THURSDAY 4.000000\n",
      "119) OCCUPATION_TYPE_Waiters/barmen staff 4.000000\n",
      "120) WEEKDAY_APPR_PROCESS_START_SUNDAY 5.000000\n",
      "121) WEEKDAY_APPR_PROCESS_START_MONDAY 5.000000\n",
      "122) FLAG_DOCUMENT_6                5.000000\n",
      "123) NAME_TYPE_SUITE_Group of people 5.000000\n",
      "124) FONDKAPREMONT_MODE_reg oper spec account 5.000000\n",
      "125) ORGANIZATION_TYPE_Security Ministries 5.000000\n",
      "126) FONDKAPREMONT_MODE_org spec account 6.000000\n",
      "127) FLAG_EMP_PHONE                 6.000000\n",
      "128) FLOORSMIN_MODE                 6.000000\n",
      "129) FLAG_DOCUMENT_5                6.000000\n",
      "130) OCCUPATION_TYPE_Core staff     6.000000\n",
      "131) OCCUPATION_TYPE_Cooking staff  6.000000\n",
      "132) ELEVATORS_MODE                 6.000000\n",
      "133) WALLSMATERIAL_MODE_Wooden      7.000000\n",
      "134) NAME_HOUSING_TYPE_Office apartment 7.000000\n",
      "135) NAME_FAMILY_STATUS_Unknown     7.000000\n",
      "136) FLAG_EMAIL                     8.000000\n",
      "137) AMT_REQ_CREDIT_BUREAU_DAY      8.000000\n",
      "138) FLOORSMAX_MODE                 8.000000\n",
      "139) NONLIVINGAPARTMENTS_MODE       8.000000\n",
      "140) FLAG_OWN_REALTY_N              8.000000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 244 is out of bounds for axis 0 with size 244",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d310b075c891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%2d) %-*s %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfi_lgbm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'importance_split'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 244 is out of bounds for axis 0 with size 244"
     ]
    }
   ],
   "source": [
    "# compute feature importances\n",
    "\n",
    "train_features = [f for f in train_input if f not in ['SK_ID_CURR','TARGET']]\n",
    "\n",
    "fi_lgbm = pd.DataFrame()\n",
    "\n",
    "fi_lgbm['importance_split'] = model.feature_importance(importance_type='split')\n",
    "fi_lgbm['importance_gain'] = model.feature_importance(importance_type='gain')\n",
    "\n",
    "feat_labels = train_input[train_features].columns[0:]\n",
    "\n",
    "indices_s = np.argsort(fi_lgbm['importance_split'])[::-1]\n",
    "indices_g = np.argsort(fi_lgbm['importance_gain'])[::-1]\n",
    "\n",
    "for f in range(train_input[train_features].shape[1]):\n",
    "    print('%2d) %-*s %f' % (f+1, 30, feat_labels[indices_s[f]], fi_lgbm['importance_split'][indices_s[f]]))\n",
    "    \n",
    "for f in range(train_input[train_features].shape[1]):\n",
    "    print('%2d) %-*s %f' % (f+1, feat_labels[indices_g[f]], fi_lgbm['importance_gain'][indices_g[f]]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw a graph of the feature importances\n",
    "\n",
    "ranking = np.argsort(-fi_lgbm['importance_split'])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(90, 90)) \n",
    "\n",
    "colors = ['red' if (x>=min(heapq.nlargest(10, fi_lgbm['importance_split'][ranking]))) else 'grey' \\\n",
    "          for x in fi_lgbm['importance_split'][ranking]]\n",
    "\n",
    "sns.barplot(x=fi_lgbm['importance_split'][ranking], y=train_input[train_features].columns.values[ranking-1], orient='h', palette=colors)\n",
    "\n",
    "ax.set_xlabel('feature importance',fontsize=70)\n",
    "ax.set_yticklabels(feat_labels[indices_s-1],fontsize=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Create a Definition of the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def of AUC\n",
    "\n",
    "def compute_auc(train_test):\n",
    "    # split the concatenated dataset into the train dataset and the test dataset\n",
    "    train_input = train_test[0: train.shape[0]].copy()\n",
    "    test_input = train_test[train.shape[0]:].copy()\n",
    "    \n",
    "    # LightGBM\n",
    "    X = train_input.drop(\"TARGET\",axis=1).values\n",
    "    y = train_input.loc[:,'TARGET'].values\n",
    "    \n",
    "    lgbm_train = lgbm.Dataset(X,y)\n",
    "    lgbm_eval = lgbm.Dataset(X, y, reference=lgbm_train)\n",
    "    lgbm_params = {'objective':'binary',\n",
    "               'boosting_type': 'rf',\n",
    "               'subsample': 0.623,\n",
    "               'colsample_bytree': 0.7,\n",
    "               'num_leaves': 127,\n",
    "               'max_depth': 8,\n",
    "               'seed': 99,\n",
    "               'bagging_freq': 1,\n",
    "               'n_jobs': 4}\n",
    "    \n",
    "    model = lgbm.train(lgbm_params, lgbm_train, valid_sets=lgbm_eval)\n",
    "    \n",
    "    # predict the test dataset\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # compute AUC\n",
    "    return roc_auc_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Preprocessing, Feature Enginnering and Fitting\n",
    "\n",
    "\n",
    "### 1st Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get top100 feature values strongly correlated with \"TARGET\"\n",
    "\n",
    "top100_correlations = corr_mat.nlargest(101, \"TARGET\")[\"TARGET\"].drop(\"TARGET\")\n",
    "\n",
    "top100_correlations_table = pd.DataFrame({\n",
    "    'correlation': top100_correlations\n",
    "})\n",
    "\n",
    "top100_correlations_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list of the 100 feature values\n",
    "\n",
    "top100_list = list(top100_correlations.index)\n",
    "\n",
    "top100_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute how many percentages each feature value includes missing values\n",
    "\n",
    "total_of_top100 = new_train_test[top100_list].isnull().sum()\n",
    "missing_ratio_of_top100 = total_of_top100 / len(new_train_test[top100_list])\n",
    "\n",
    "missing_ratio_table_of_top100 = pd.DataFrame({\n",
    "    'Total': total_of_top100,\n",
    "    'missing_ratio': missing_ratio_of_top100\n",
    "})\n",
    "\n",
    "missing_ratio_table_of_top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get top10 feature values whose missing ratio is in the above top100 list\n",
    "\n",
    "top10_missing_ratio = missing_ratio_table_of_top100.nlargest(11, \"missing_ratio\")[\"missing_ratio\"]\n",
    "\n",
    "top10_missing_ratio_table = pd.DataFrame({\n",
    "    'missing_ratio': top10_missing_ratio\n",
    "})\n",
    "\n",
    "top10_missing_ratio_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of \"OWN_CAR_AGE\"\n",
    "\n",
    "new_train_test.OWN_CAR_AGE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_train_test.OWN_CAR_AGE[new_train_test.OWN_CAR_AGE==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio of 0 is {}%\".format(len(new_train_test.OWN_CAR_AGE[new_train_test.OWN_CAR_AGE==0]) / len(new_train_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - I guess that many people who did not own theirown car tended not to answer the question of the \"OWN_CAR_AGE\" column while some other people(only 0.6%) answered 0. This is because even 66% are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of \"AMT_REQ_CREDIT_BUREAU_YEAR\"\n",
    "\n",
    "new_train_test.AMT_REQ_CREDIT_BUREAU_YEAR.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - This feature value means the number of enquiries to Credit Bureau about the client one day year (excluding last 3 months before application). Therefore, I guess missing values might equal to 0, since the number of enquiries shold be counted if people did so.\n",
    "    \n",
    "    - This prediction can apply to same kind of other 5 feature values such as \"AMT_REQ_CREDIT_BUREAU_QRT\" and \"AMT_REQ_CREDIT_BUREAU_MON\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# handle the missing values of the 7 columns by changing them to 0s\n",
    "\n",
    "edit_missing_values = {\"OWN_CAR_AGE\":0,\"AMT_REQ_CREDIT_BUREAU_YEAR\":0,\"AMT_REQ_CREDIT_BUREAU_QRT\":0,\"AMT_REQ_CREDIT_BUREAU_MON\":0,\n",
    "                       \"AMT_REQ_CREDIT_BUREAU_DAY\":0,\"AMT_REQ_CREDIT_BUREAU_HOUR\":0,\"AMT_REQ_CREDIT_BUREAU_WEEK\":0}\n",
    "\n",
    "edit_train_test = new_train_test.fillna(value=edit_missing_values)\n",
    "\n",
    "edit_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute AUC\n",
    "\n",
    "compute_auc(edit_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The score is almost same with the previous one. (Exactly speaking, it slightly goes down.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Try by Focusing on the Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of \"ENTRANCES_MODE\"\n",
    "\n",
    "new_train_test.ENTRANCES_MODE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computer the missing ratio\n",
    "\n",
    "print(\"missing ratio is {}%\".format(new_train_test.ENTRANCES_MODE.isnull().sum() / len(new_train_test.ENTRANCES_MODE) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get top100 feature values strongly correlated with \"ENTRANCES_MODE\"\n",
    "\n",
    "correlations_ENTRANCES_MODE = corr_mat.nlargest(101, \"ENTRANCES_MODE\")[\"ENTRANCES_MODE\"].drop(\"ENTRANCES_MODE\")\n",
    "\n",
    "correlations_ENTRANCES_MODE_table = pd.DataFrame({\n",
    "    'correlation': correlations_ENTRANCES_MODE\n",
    "})\n",
    "\n",
    "correlations_ENTRANCES_MODE_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The correlations of \"ENTRANCES_MODE\" with the top2 feature values, \"ENTRANCES_MEDI\" and \"ENTRANCES_AVG\" are too high. So, I choose the third feature value, \"BASEMENTAREA_MODE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between the missing values of \"ENTRANCES_MODE\" and \"BASEMENTAREA_MODE\"\n",
    "\n",
    "indexer_1 = new_train_test['ENTRANCES_MODE'].isnull()\n",
    "missing_1 = new_train_test['ENTRANCES_MODE'].copy()\n",
    "missing_1[indexer_1] = -1\n",
    "missing_1[~indexer_1] = 1\n",
    "df_missing_1 = pd.DataFrame(missing_1)\n",
    "df_missing_1.columns = ['MISSING']\n",
    "\n",
    "\n",
    "df_ENTRANCES_MODE = new_train_test['ENTRANCES_MODE'].copy()\n",
    "df_ENTRANCES_MODE[indexer_1] = 0.145471   # the mean of \"ENTRANCES_MODE\"\n",
    "df_ENTRANCES_MODE = pd.DataFrame(df_ENTRANCES_MODE)\n",
    "df_ENTRANCES_MODE.columns = ['ENTRANCES_MODE_MEANS']\n",
    "\n",
    "\n",
    "df_ENTRANCES_MODE_missing = pd.concat([df_missing_1, df_ENTRANCES_MODE, new_train_test], axis=1)\n",
    "\n",
    "ax = sns.lmplot(x='BASEMENTAREA_MODE', y='ENTRANCES_MODE_MEANS', fit_reg = False, data=df_ENTRANCES_MODE_missing, hue='MISSING', \n",
    "                palette='Greys_r', size=7, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - There are no strong correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# handle the missing values of \"ENTRANCES_MODE\" by changing the missing values to the medians\n",
    "\n",
    "edit_columns = []\n",
    "for _ in train.iloc[:,48:94].columns:\n",
    "    edit_columns.append(_)\n",
    "\n",
    "edit_columns.remove(\"FONDKAPREMONT_MODE\")\n",
    "edit_columns.remove(\"HOUSETYPE_MODE\")\n",
    "edit_columns.remove('WALLSMATERIAL_MODE')\n",
    "edit_columns.remove('EMERGENCYSTATE_MODE')\n",
    "\n",
    "for _ in edit_columns:\n",
    "    edit2_train_test = new_train_test.fillna(new_train_test[_].mean())\n",
    "\n",
    "edit2_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute AUC\n",
    "\n",
    "compute_auc(edit2_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The score goes down slightly again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the contents of \"EXT_SOURCE_1\"\n",
    "\n",
    "new_train_test.EXT_SOURCE_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get top 50 feature values strongly correlated with \"EXT_SOURCE_1\"\n",
    "\n",
    "EXT_SOURCE_1_correlations = corr_mat.nlargest(51, \"EXT_SOURCE_1\")[\"EXT_SOURCE_1\"].drop(\"EXT_SOURCE_1\")\n",
    "\n",
    "EXT_SOURCE_1_correlations_table = pd.DataFrame({\n",
    "    'correlation': EXT_SOURCE_1_correlations\n",
    "})\n",
    "\n",
    "EXT_SOURCE_1_correlations_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - There are no plus storng correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare \"EXT_SOURCE_1\" with \"TARGET\"\n",
    "\n",
    "new_train_test.EXT_SOURCE_1[new_train_test.TARGET==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_test.EXT_SOURCE_1[new_train_test.TARGET==0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - I need to take the same number of samples from \"EXT_SOURCE_1\" whose \"TARGET\" values is 0 with the number of \"EXT_SOURCE_1\" whose \"TARGET\" values is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the number of 2 kinds of samples\n",
    "\n",
    "new_train_test.EXT_SOURCE_1[new_train_test.TARGET==0].sample(n=25000).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The mean of the samples whose \"TARGET\" value is 1 smaller than the samples whose \"TARGET\" value is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature value based on \"EXT_SOURCE_1\"\n",
    "\n",
    "# add a spare column\n",
    "new_train_test[\"ADD_EXT_SOURCE_1\"] = float(0)\n",
    "\n",
    "# substitute 1 to the samples whose mean is less than the mean of the 2 kinds of samples whose \"TARGET\" values is 0 or 1.\n",
    "new_train_test.loc[new_train_test.EXT_SOURCE_1<(new_train_test.EXT_SOURCE_1[new_train_test.TARGET==1].mean()+ \\\n",
    "                                                new_train_test.EXT_SOURCE_1[new_train_test.TARGET==0].sample(n=25000).mean())/2, \n",
    "                   \"ADD_EXT_SOURCE_1\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature value based on \"EXT_SOURCE_2\"\n",
    "\n",
    "# add a spare column\n",
    "new_train_test[\"ADD_EXT_SOURCE_2\"] = float(0)\n",
    "\n",
    "# substitute 1 to the samples whose mean is less than the mean of the 2 kinds of samples whose \"TARGET\" values is 0 or 1.\n",
    "new_train_test.loc[new_train_test.EXT_SOURCE_2<(new_train_test.EXT_SOURCE_2[new_train_test.TARGET==1].mean()+ \\\n",
    "                                                new_train_test.EXT_SOURCE_2[new_train_test.TARGET==0].sample(n=25000).mean())/2, \n",
    "                   \"ADD_EXT_SOURCE_2\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature value based on \"EXT_SOURCE_3\"\n",
    "\n",
    "# add a spare column\n",
    "new_train_test[\"ADD_EXT_SOURCE_3\"] = float(0)\n",
    "\n",
    "# substitute 1 to the samples whose mean is less than the mean of the 2 kinds of samples whose \"TARGET\" values is 0 or 1.\n",
    "new_train_test.loc[new_train_test.EXT_SOURCE_3<(new_train_test.EXT_SOURCE_3[new_train_test.TARGET==1].mean()+ \\\n",
    "                                                new_train_test.EXT_SOURCE_3[new_train_test.TARGET==0].sample(n=25000).mean())/2, \n",
    "                   \"ADD_EXT_SOURCE_3\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute AUC\n",
    "\n",
    "compute_auc(new_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The score goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submittion Process From Here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# predict the test dataset\n",
    "y_test_pred2 = model2.predict(test_input2.values, num_iteration=model2.best_iteration)\n",
    "\n",
    "# submittion the result\n",
    "sub = pd.read_csv('\"Home Credit Default Risk\"_sample_submission.csv')\n",
    "\n",
    "sub['TARGET'] = y_test_pred2\n",
    "\n",
    "sub.to_csv('\"Home Credit Default Risk\"_pipeline5.csv', index=False)   # I need to set a file name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
