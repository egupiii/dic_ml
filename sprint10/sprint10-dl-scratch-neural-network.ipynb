{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to create a class of 3 layers of neural network for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "\n",
    "print(X_train.shape)   # (60000, 28, 28)\n",
    "print(X_test.shape)   # (10000, 28, 28)\n",
    "print(X_train[0].dtype)   # uint8\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten\n",
    "\n",
    "<br />\n",
    "\n",
    "I will transform the shape (1, 28, 28) of each image to (1, 784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# X_train[index]: (784,)\n",
    "# image: (28, 28)\n",
    "\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n",
      "    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n",
      "   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n",
      "   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n",
      "   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n",
      "   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n",
      "   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
      "    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n",
      "   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n",
      "    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n",
      "   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n",
      "   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n",
      "   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n",
      "   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n",
      "   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n",
      "   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n",
      "   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n",
      "   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n",
      "    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]\n",
      " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
      "  -105.35]]\n"
     ]
    }
   ],
   "source": [
    "# Plot\n",
    "\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "# Change it to float\n",
    "image = image.astype(np.float)\n",
    "# Create minus fractional value intentionally\n",
    "image -= 105.35\n",
    "\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()\n",
    "\n",
    "# Check the values\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Transform unit8 to float\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.max())   # 1.0\n",
    "print(X_train.min())   # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# One hot encoder\n",
    "\n",
    "# Initialize\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Fit\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "\n",
    "# Transform\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(y_train.shape)   # (60000,)\n",
    "print(y_train_one_hot.shape)   # (60000, 10)\n",
    "print(y_train_one_hot.dtype)   # float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 784)\n",
      "(12000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Split the train dataset\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)   # (48000, 784)\n",
    "print(X_val.shape)   # (12000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Create a Class of Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class to get a mini-batch\n",
    "\n",
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      Train dataset\n",
    "    \n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "      Correct values\n",
    "    \n",
    "    batch_size : int\n",
    "      Size of batch\n",
    "    \n",
    "    seed : int\n",
    "      Seed of random numbers of Numpy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forwardpropagation\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to create a forwardpropagation method of 3 layers of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class of a forwardpropagation method of 3 layers of neural network\n",
    "\n",
    "class Forwardpropagation():\n",
    "    \"\"\"\n",
    "    Forwardpropagation method\n",
    "    \"\"\"\n",
    "    \n",
    "    def layer_processing(self, X, W, B):\n",
    "        \"\"\"\n",
    "        Processing of a layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Feature vector\n",
    "\n",
    "        W : ndarray, shape (n_features, ith n_nodes)\n",
    "            Weight of ith layer\n",
    "\n",
    "        B : ndarray, shape (ith n_nodes,)\n",
    "            Bias of ith layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.dot(X, W) + B\n",
    "    \n",
    "    \n",
    "    def activation_func(self, A, act_func):\n",
    "        \"\"\"\n",
    "        Activation function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sigmoid function\n",
    "        if act_func == \"sigmoid\":\n",
    "            return 1 / (1+np.exp(-A))\n",
    "        \n",
    "        # tanh\n",
    "        if act_func == \"tanh\":\n",
    "            return np.tanh(A)\n",
    "    \n",
    "    \n",
    "    def softmax_func(self, A3):\n",
    "        \"\"\"\n",
    "        Softmax function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \"\"\"\n",
    "        \n",
    "        A3 = A3 - np.max(A3)\n",
    "        \n",
    "        return np.exp(A3) / np.sum(np.exp(A3), axis=1)[:,np.newaxis]\n",
    "    \n",
    "    \n",
    "    # Objective function\n",
    "    def cross_entropy_loss(self, y, X3):\n",
    "        \"\"\"\n",
    "        Cross entropy loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape ()\n",
    "            Cross entropy loss\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.sum(-y * np.log(X3), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to create a backpropagation method of 3 layers of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class of a backpropagation method of 3 layers of neural network\n",
    "\n",
    "class Backpropagation():\n",
    "    \"\"\"\n",
    "    Backpropagation method\n",
    "    \"\"\"\n",
    "    \n",
    "    def third_layer(self, y, X2, X3, W3):\n",
    "        # Gradient of loss for A3\n",
    "        delta1 = X3 - y\n",
    "        # Gradient of loss for B3\n",
    "        B3_grad = delta1\n",
    "        # Gradient of loss for W3\n",
    "        W3_grad = np.dot(X2.T, delta1)\n",
    "        # Gradient of loss for Z2\n",
    "        Z2_grad = np.dot(delta1, W3.T)\n",
    "        \n",
    "        return delta1, B3_grad, W3_grad, Z2_grad\n",
    "    \n",
    "    \n",
    "    def second_layer(self, X1, A2, W2, W3, delta1, Z2_grad):\n",
    "        # Gradient of loss for A2\n",
    "        delta2 = (1-np.tanh(A2)**2) * Z2_grad        \n",
    "        # Gradient of loss for B2\n",
    "        B2_grad = delta2\n",
    "        # Gradient of loss for W2\n",
    "        W2_grad = np.dot(X1.T, delta2)\n",
    "        # Gradient of loss for Z1\n",
    "        Z1_grad = np.dot(delta2, W2.T)\n",
    "        \n",
    "        return delta2, B2_grad, W2_grad, Z1_grad\n",
    "    \n",
    "    \n",
    "    def first_layer(self, X, A1, W2, delta2, Z1_grad):\n",
    "        # Gradient of loss for A1\n",
    "        delta3 = (1-np.tanh(A1)**2) * Z1_grad\n",
    "        # Gradient of loss for B1\n",
    "        B1_grad = delta3\n",
    "        # Gradient of loss for W1\n",
    "        W1_grad = np.dot(X.T, delta3)\n",
    "        \n",
    "        return delta3, B1_grad, W1_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class of neural network classifier\n",
    "\n",
    "class ScratchSimpleNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    Implement simple 3 layers of neural network classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_epoch : int\n",
    "        Number of epochs\n",
    "    \n",
    "    act_func : str\n",
    "        Name of activation function\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    n_features : int\n",
    "        Number of features\n",
    "    \n",
    "    n_nodes1 : int\n",
    "        Number of nodes of 1st layer\n",
    "    \n",
    "    n_nodes2 : int\n",
    "        Number of nodes of 2nd layer\n",
    "    \n",
    "    n_output : int\n",
    "        Number of classes of output (Number of nodes of 3rd layer)\n",
    "    \n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \n",
    "    verbose : bool\n",
    "        True if outputting learning process\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : ndarray, shape (self.epoch,)\n",
    "        Records of loss on train dataset\n",
    "    \n",
    "    val_loss : ndarray, shape (self.epoch,)\n",
    "        Records of loss on validation dataset\n",
    "    \n",
    "    W1 : ndarray, shape (n_features, n_nodes1)\n",
    "        Weight of 1st layer\n",
    "    \n",
    "    B1 : ndarray, shape (n_nodes1,)\n",
    "        Bias of 1st layer\n",
    "    \n",
    "    W2 : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight of 2st layer\n",
    "    \n",
    "    B2 : ndarray, shape (n_nodes2,)\n",
    "        Bias of 2st layer\n",
    "    \n",
    "    W3 : ndarray, shape (n_nodes2, n_nodes3)\n",
    "        Weight of 3st layer\n",
    "    \n",
    "    B3 : ndarray, shape (n_nodes3,)\n",
    "        Bias of 3st layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_epoch, batch_size, act_func, lr, n_features, n_nodes1, n_nodes2, n_output, sigma, verbose=True):\n",
    "        # Record hyperparameters as attribute\n",
    "        self.epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.act_func = act_func\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Prepare arrays for recording loss\n",
    "        self.loss = np.zeros(self.epoch)\n",
    "        self.val_loss = np.zeros(self.epoch)\n",
    "        \n",
    "        # Initialize\n",
    "        fp = Forwardpropagation()\n",
    "        bp = Backpropagation()\n",
    "        \n",
    "        # Get an initial values\n",
    "        self.W1 = sigma * np.random.randn(n_features, n_nodes1)\n",
    "        self.B1 = sigma * np.random.randn(n_nodes1)[np.newaxis,:]\n",
    "        self.W2 = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        self.B2 = sigma * np.random.randn(n_nodes2)[np.newaxis,:]\n",
    "        self.W3 = sigma * np.random.randn(n_nodes2, n_output)\n",
    "        self.B3 = sigma * np.random.randn(n_output)[np.newaxis,:]\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Correct values of train dataset\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation dataset\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples, )\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize\n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            get_mini_batch_val = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "        \n",
    "        # Initialize\n",
    "        fp = Forwardpropagation()\n",
    "        bp = Backpropagation()\n",
    "        \n",
    "        # Fit\n",
    "        if self.verbose:\n",
    "            count = 0\n",
    "        for i in range(self.epoch):\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                for ((mini_X_train, mini_y_train), (mini_X_val_train, mini_y_val_train)) in zip(get_mini_batch, get_mini_batch_val):\n",
    "                    # Forwardpropagation\n",
    "                    # 1st layer\n",
    "                    # Processing of the layer\n",
    "                    A1 = fp.layer_processing(mini_X_train, self.W1, self.B1)\n",
    "                    # Activation function\n",
    "                    X1 = fp.activation_func(A1, self.act_func)\n",
    "\n",
    "                    # 2nd layer\n",
    "                    # Processing of the layer\n",
    "                    A2 = fp.layer_processing(X1, self.W2, self.B2)\n",
    "                    # Activation function\n",
    "                    X2 = fp.activation_func(A2, self.act_func)\n",
    "\n",
    "                    # 3rd layer (Softmax)\n",
    "                    # Processing of the layer\n",
    "                    A3 = fp.layer_processing(X2, self.W3, self.B3)\n",
    "                    # Activation function\n",
    "                    X3 = fp.softmax_func(A3)\n",
    "\n",
    "                    # 4th layer (Cross entropy loss)\n",
    "                    if self.verbose:\n",
    "                        L = fp.cross_entropy_loss(mini_y_train, X3)\n",
    "\n",
    "\n",
    "                    # Backpropagation\n",
    "                    # 3rd layer\n",
    "                    delta1, B3_grad, W3_grad, Z2_grad = bp.third_layer(mini_y_train, X2, X3, self.W3)\n",
    "                    # Update weight and bias\n",
    "                    self.B3 = self.B3 - self.lr*np.average(B3_grad, axis=0)\n",
    "                    self.W3 = self.W3 - self.lr*W3_grad/W3_grad.shape[1]\n",
    "\n",
    "                    # 2nd layer\n",
    "                    delta2, B2_grad, W2_grad, Z1_grad = bp.second_layer(X1, A2, self.W2, self.W3, delta1, Z2_grad)\n",
    "                    # Update weight and bias\n",
    "                    self.B2 = self.B2 - self.lr*np.average(B2_grad, axis=0)\n",
    "                    self.W2 = self.W2 - self.lr*W2_grad/W2_grad.shape[1]\n",
    "\n",
    "                    # 1st layer\n",
    "                    delta3, B1_grad, W1_grad = bp.first_layer(mini_X_train, A1, self.W2, delta2, Z1_grad)\n",
    "                    # Update weight and bias\n",
    "                    self.B1 = self.B1 - self.lr*np.average(B1_grad, axis=0)\n",
    "                    self.W1 = self.W1 - self.lr*W1_grad/W1_grad.shape[1]\n",
    "                    \n",
    "                    \n",
    "                    # Validation data\n",
    "                    # Forwardpropagation\n",
    "                    # 1st layer\n",
    "                    # Processing of the layer\n",
    "                    A1 = fp.layer_processing(mini_X_val_train, self.W1, self.B1)\n",
    "                    # Activation function\n",
    "                    X1 = fp.activation_func(A1, self.act_func)\n",
    "\n",
    "                    # 2nd layer\n",
    "                    # Processing of the layer\n",
    "                    A2 = fp.layer_processing(X1, self.W2, self.B2)\n",
    "                    # Activation function\n",
    "                    X2 = fp.activation_func(A2, self.act_func)\n",
    "\n",
    "                    # 3rd layer (Softmax)\n",
    "                    # Processing of the layer\n",
    "                    A3 = fp.layer_processing(X2, self.W3, self.B3)\n",
    "                    # Activation function\n",
    "                    X3 = fp.softmax_func(A3)\n",
    "\n",
    "                    # 4th layer (Cross entropy loss)\n",
    "                    if self.verbose:\n",
    "                        L_val = fp.cross_entropy_loss(mini_y_val_train, X3)\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                    # Forwardpropagation\n",
    "                    # 1st layer\n",
    "                    # Processing of the layer\n",
    "                    A1 = fp.layer_processing(mini_X_train, self.W1, self.B1)\n",
    "                    # Activation function\n",
    "                    X1 = fp.activation_func(A1, self.act_func)\n",
    "\n",
    "                    # 2nd layer\n",
    "                    # Processing of the layer\n",
    "                    A2 = fp.layer_processing(X1, self.W2, self.B2)\n",
    "                    # Activation function\n",
    "                    X2 = fp.activation_func(A2, self.act_func)\n",
    "\n",
    "                    # 3rd layer (Softmax)\n",
    "                    # Processing of the layer\n",
    "                    A3 = fp.layer_processing(X2, self.W3, self.B3)\n",
    "                    # Activation function\n",
    "                    X3 = fp.softmax_func(A3)\n",
    "\n",
    "                    # 4th layer (Cross entropy loss)\n",
    "                    if self.verbose:\n",
    "                        L = fp.cross_entropy_loss(mini_y_train, X3)\n",
    "\n",
    "\n",
    "                    # Backpropagation\n",
    "                    # 3rd layer\n",
    "                    delta1, B3_grad, W3_grad, Z2_grad = bp.third_layer(mini_y_train, X2, X3, self.W3)\n",
    "                    # Update weight and bias\n",
    "                    self.B3 = self.B3 - self.lr*np.average(B3_grad, axis=0)\n",
    "                    self.W3 = self.W3 - self.lr*W3_grad/W3_grad.shape[1]\n",
    "\n",
    "                    # 2nd layer\n",
    "                    delta2, B2_grad, W2_grad, Z1_grad = bp.second_layer(X1, A2, self.W2, self.W3, delta1, Z2_grad)\n",
    "                    # Update weight and bias\n",
    "                    self.B2 = self.B2 - self.lr*np.average(B2_grad, axis=0)\n",
    "                    self.W2 = self.W2 - self.lr*W2_grad/W2_grad.shape[1]\n",
    "\n",
    "                    # 1st layer\n",
    "                    delta3, B1_grad, W1_grad = bp.first_layer(mini_X_train, A1, self.W2, delta2, Z1_grad)\n",
    "                    # Update weight and bias\n",
    "                    self.B1 = self.B1 - self.lr*np.average(B1_grad, axis=0)\n",
    "                    self.W1 = self.W1 - self.lr*W1_grad/W1_grad.shape[1]\n",
    "            \n",
    "            \n",
    "            # Output learning process if verbose is True\n",
    "            if self.verbose:\n",
    "                self.loss[count] = sum(L) / self.batch_size\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    self.val_loss[count] = sum(L_val) / self.batch_size\n",
    "                    print(\"{0}th loss: {1}, val_loss: {2}\".format(count+1, self.loss[count], self.val_loss[count]))\n",
    "                else:\n",
    "                    print(self.loss[count])\n",
    "                count += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict by neural network classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples, 1)\n",
    "            Results of prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize\n",
    "        fp = Forwardpropagation()\n",
    "        \n",
    "        # Forwardpropagation\n",
    "        # 1st layer\n",
    "        # Processing of the layer\n",
    "        A1 = fp.layer_processing(X, self.W1, self.B1)\n",
    "        # Activation function\n",
    "        X1 = fp.activation_func(A1, self.act_func)\n",
    "\n",
    "        # 2nd layer\n",
    "        # Processing of the layer\n",
    "        A2 = fp.layer_processing(X1, self.W2, self.B2)\n",
    "        # Activation function\n",
    "        X2 = fp.activation_func(A2, self.act_func)\n",
    "\n",
    "        # 3rd layer (Softmax)\n",
    "        # Processing of the layer\n",
    "        A3 = fp.layer_processing(X2, self.W3, self.B3)\n",
    "        # Activation function\n",
    "        X3 = fp.softmax_func(A3)\n",
    "        \n",
    "        return np.argmax(X3, axis=1)\n",
    "    \n",
    "    \n",
    "    # Plot learning records\n",
    "    def plot_learning_record(self):\n",
    "        \"\"\"\n",
    "        Plot learning records.\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(facecolor=\"azure\", edgecolor=\"coral\")\n",
    "        \n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        \n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Compute index values\n",
    "    def compute_index_values(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Index values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Return index values\n",
    "        print(\"accuracy score:\", accuracy_score(y, y_pred))\n",
    "    \n",
    "    \n",
    "    def plot_misclassification(self, X_val, y_val, y_pred):\n",
    "        \"\"\"\n",
    "        Plot results of misclassification. Show \"Results of prediction/Corrects\" above images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Results of prediction\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples,)\n",
    "            Correct labels of validation data\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Number of results I want to plot\n",
    "        num = 36\n",
    "\n",
    "        true_false = y_pred==y_val\n",
    "        false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "        if false_list.shape[0] < num:\n",
    "            num = false_list.shape[0]\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "        for i in range(num):\n",
    "            ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "            ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "            ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "nn = ScratchSimpleNeuralNetrowkClassifier(100, 10, \"tanh\", 0.01, 784, 400, 200, 10, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th loss: 2.305999736287943, val_loss: 2.2876000020287797\n",
      "2th loss: 2.2951686195910623, val_loss: 2.272890988626444\n",
      "3th loss: 2.271489353505651, val_loss: 2.2484152264040222\n",
      "4th loss: 2.2183305820461645, val_loss: 2.192919773931805\n",
      "5th loss: 2.0866648482492063, val_loss: 2.0516488064396805\n",
      "6th loss: 1.8075162974950154, val_loss: 1.7759673026857417\n",
      "7th loss: 1.4538308371338493, val_loss: 1.5259884617736603\n",
      "8th loss: 1.1463938493155195, val_loss: 1.4079417741717983\n",
      "9th loss: 0.9280653407589428, val_loss: 1.383188233596287\n",
      "10th loss: 0.7833389878372976, val_loss: 1.3934201915300806\n",
      "11th loss: 0.6814492519911003, val_loss: 1.4058970360940883\n",
      "12th loss: 0.6025347252438547, val_loss: 1.4102843309418214\n",
      "13th loss: 0.5369169011670648, val_loss: 1.4067933185048982\n",
      "14th loss: 0.4805365677263119, val_loss: 1.3989683554252121\n",
      "15th loss: 0.4319408869183697, val_loss: 1.3901681366358098\n",
      "16th loss: 0.39059401212886014, val_loss: 1.3823750855264962\n",
      "17th loss: 0.35598454382355593, val_loss: 1.3763078449509973\n",
      "18th loss: 0.32733176687542265, val_loss: 1.3719480244482294\n",
      "19th loss: 0.3036615580877832, val_loss: 1.368978435300567\n",
      "20th loss: 0.28398087331571814, val_loss: 1.3670288195650588\n",
      "21th loss: 0.2674027253645664, val_loss: 1.3657812299720509\n",
      "22th loss: 0.2531995163768877, val_loss: 1.3649990850332259\n",
      "23th loss: 0.24080930778304696, val_loss: 1.3645216289654314\n",
      "24th loss: 0.2298185762651932, val_loss: 1.3642458803327533\n",
      "25th loss: 0.21993533103930143, val_loss: 1.364107213262463\n",
      "26th loss: 0.2109602294017415, val_loss: 1.3640635703791995\n",
      "27th loss: 0.2027600561252275, val_loss: 1.3640845404873654\n",
      "28th loss: 0.1952458839481894, val_loss: 1.3641446464941003\n",
      "29th loss: 0.1883566604900151, val_loss: 1.3642196788784386\n",
      "30th loss: 0.18204783537701036, val_loss: 1.3642850530719943\n",
      "31th loss: 0.17628403972863488, val_loss: 1.3643154445852965\n",
      "32th loss: 0.17103469133793908, val_loss: 1.3642851718279476\n",
      "33th loss: 0.16627154072483832, val_loss: 1.364168945391944\n",
      "34th loss: 0.16196741874358095, val_loss: 1.3639427161155435\n",
      "35th loss: 0.15809568429369344, val_loss: 1.363584449750437\n",
      "36th loss: 0.15463005517025166, val_loss: 1.363074734797474\n",
      "37th loss: 0.15154463137469507, val_loss: 1.3623971889364785\n",
      "38th loss: 0.1488140004758056, val_loss: 1.36153866803797\n",
      "39th loss: 0.14641336364571367, val_loss: 1.3604893031671146\n",
      "40th loss: 0.1443186504538478, val_loss: 1.3592424000596077\n",
      "41th loss: 0.14250660807178508, val_loss: 1.3577942368202565\n",
      "42th loss: 0.14095486088333467, val_loss: 1.3561437925494872\n",
      "43th loss: 0.13964194236047583, val_loss: 1.3542924345825413\n",
      "44th loss: 0.13854730411167032, val_loss: 1.3522435864743227\n",
      "45th loss: 0.1376513082656284, val_loss: 1.3500023936007355\n",
      "46th loss: 0.1369352094853226, val_loss: 1.3475753986843462\n",
      "47th loss: 0.1363811323500534, val_loss: 1.3449702358352023\n",
      "48th loss: 0.13597204888603853, val_loss: 1.3421953488116107\n",
      "49th loss: 0.13569175986005308, val_loss: 1.33925973704568\n",
      "50th loss: 0.13552488220671863, val_loss: 1.3361727313998863\n",
      "51th loss: 0.13545684373195782, val_loss: 1.3329438004732885\n",
      "52th loss: 0.13547388509563996, val_loss: 1.3295823874248751\n",
      "53th loss: 0.13556306808542837, val_loss: 1.3260977766197097\n",
      "54th loss: 0.13571228840112554, val_loss: 1.3224989888560512\n",
      "55th loss: 0.13591029061253482, val_loss: 1.3187947034543557\n",
      "56th loss: 0.13614668265709826, val_loss: 1.3149932050647057\n",
      "57th loss: 0.13641194721081767, val_loss: 1.3111023526800658\n",
      "58th loss: 0.136697447480905, val_loss: 1.3071295680434036\n",
      "59th loss: 0.13699542539472054, val_loss: 1.3030818404258326\n",
      "60th loss: 0.13729899074333812, val_loss: 1.298965744646897\n",
      "61th loss: 0.13760210051492427, val_loss: 1.294787469215957\n",
      "62th loss: 0.13789952835452454, val_loss: 1.290552851594243\n",
      "63th loss: 0.13818682474804886, val_loss: 1.2862674177990931\n",
      "64th loss: 0.1384602690947934, val_loss: 1.2819364238753548\n",
      "65th loss: 0.13871681526541574, val_loss: 1.27756489711808\n",
      "66th loss: 0.13895403251901867, val_loss: 1.273157675318426\n",
      "67th loss: 0.13917004376959152, val_loss: 1.2687194426953954\n",
      "68th loss: 0.13936346315955342, val_loss: 1.2642547615483868\n",
      "69th loss: 0.13953333473953936, val_loss: 1.2597680990041122\n",
      "70th loss: 0.13967907379941719, val_loss: 1.2552638485269123\n",
      "71th loss: 0.13980041207973296, val_loss: 1.2507463461098247\n",
      "72th loss: 0.13989734774860152, val_loss: 1.2462198812655578\n",
      "73th loss: 0.1399701006861039, val_loss: 1.2416887030946644\n",
      "74th loss: 0.14001907330059776, val_loss: 1.2371570218276944\n",
      "75th loss: 0.1400448168261919, val_loss: 1.2326290063239564\n",
      "76th loss: 0.140048002828646, val_loss: 1.2281087780669695\n",
      "77th loss: 0.14002939948229404, val_loss: 1.2236004022301357\n",
      "78th loss: 0.13998985207243458, val_loss: 1.2191078763996\n",
      "79th loss: 0.1399302671210715, val_loss: 1.2146351175380388\n",
      "80th loss: 0.1398515995214455, val_loss: 1.2101859477559154\n",
      "81th loss: 0.13975484208960526, val_loss: 1.205764079428186\n",
      "82th loss: 0.13964101699011394, val_loss: 1.2013731001565622\n",
      "83th loss: 0.1395111685590505, val_loss: 1.1970164580322453\n",
      "84th loss: 0.13936635712300435, val_loss: 1.1926974476032621\n",
      "85th loss: 0.1392076534912596, val_loss: 1.1884191968958469\n",
      "86th loss: 0.1390361338747183, val_loss: 1.1841846557820268\n",
      "87th loss: 0.13885287505561245, val_loss: 1.179996585927258\n",
      "88th loss: 0.1386589496942419, val_loss: 1.175857552493679\n",
      "89th loss: 0.13845542171137856, val_loss: 1.1717699177176382\n",
      "90th loss: 0.1382433417270998, val_loss: 1.1677358364256842\n",
      "91th loss: 0.1380237425687843, val_loss: 1.163757253502165\n",
      "92th loss: 0.13779763488343577, val_loss: 1.15983590327509\n",
      "93th loss: 0.13756600290342974, val_loss: 1.15597331074553\n",
      "94th loss: 0.1373298004214199, val_loss: 1.152170794550484\n",
      "95th loss: 0.13708994703082117, val_loss: 1.1484294715200247\n",
      "96th loss: 0.13684732468436459, val_loss: 1.144750262667071\n",
      "97th loss: 0.1366027746159866, val_loss: 1.1411339004321186\n",
      "98th loss: 0.13635709466193444, val_loss: 1.1375809369956715\n",
      "99th loss: 0.1361110370064655, val_loss: 1.1340917534672659\n",
      "100th loss: 0.13586530636672417, val_loss: 1.1306665697614844\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "\n",
    "nn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "y_pred = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "\n",
    "nn.plot_learning_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Compute Evaluation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.9142\n"
     ]
    }
   ],
   "source": [
    "nn.compute_index_values(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Option] Check Misclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kazukiegusa/.pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/ipykernel_launcher.py:347: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.plot_misclassification(X_val, y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
