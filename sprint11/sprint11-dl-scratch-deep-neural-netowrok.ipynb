{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to rewrite codes of 3 layers of neural network that I created on the previous sprint to expand them to any structures by using classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    Implement neural network classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_epoch : int\n",
    "        Number of epochs\n",
    "    \n",
    "    batch_size : int\n",
    "        Size of batch\n",
    "    \n",
    "    verbose : bool\n",
    "        True if outputting learning process\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : list\n",
    "        List of arrays of records of loss on train dataset\n",
    "    \n",
    "    val_loss : list\n",
    "        List of arrays of records of loss on validation dataset\n",
    "    \n",
    "    layers : list\n",
    "        List of layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_epoch, batch_size, verbose=True):\n",
    "        # Record hyperparameters as attribute\n",
    "        self.epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.layers = []\n",
    "    \n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers += [layer]\n",
    "    \n",
    "    \n",
    "    def forward_layer(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def backward_layer(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Correct values of train dataset\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation dataset\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples, )\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit\n",
    "        if self.verbose:\n",
    "            count = 0\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            # Initialize\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            \n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                get_mini_batch_val = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "                \n",
    "                for ((mini_X_train, mini_y_train), (mini_X_val_train, mini_y_val_train)) in zip(get_mini_batch, get_mini_batch_val):\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    Z3_val = self.forward_layer(mini_X_val_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                        L_val = loss.cross_entropy_loss(mini_y_val_train, Z3_val)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "                    dX_val = self.backward_layer(mini_y_val_train)\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "            \n",
    "            \n",
    "            # Output learning process if verbose is True\n",
    "            if self.verbose:\n",
    "                self.loss += [sum(L) / self.batch_size]\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    self.val_loss += [sum(L_val) / self.batch_size]\n",
    "                    print(\"{0}ep loss: {1}, val_loss: {2}\".format(count+1, self.loss[count], self.val_loss[count]))\n",
    "                else:\n",
    "                    print(self.loss[count])\n",
    "                count += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict by neural network classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples, 1)\n",
    "            Results of prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        Z3 = self.forward_layer(X)\n",
    "        \n",
    "        return np.argmax(Z3, axis=1)\n",
    "    \n",
    "    \n",
    "    def plot_learning_record(self):\n",
    "        \"\"\"\n",
    "        Plot learning records.\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(facecolor=\"azure\", edgecolor=\"coral\")\n",
    "        \n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        \n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def compute_index_values(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Index values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"accuracy score:\", accuracy_score(y, y_pred))\n",
    "    \n",
    "    \n",
    "    def plot_misclassification(self, X_val, y_val, y_pred):\n",
    "        \"\"\"\n",
    "        Plot results of misclassification. Show \"Results of prediction/Corrects\" above images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Results of prediction\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples,)\n",
    "            Correct labels of validation data\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Number of results I want to plot\n",
    "        num = 36\n",
    "\n",
    "        true_false = y_pred==y_val\n",
    "        false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "        if false_list.shape[0] < num:\n",
    "            num = false_list.shape[0]\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "        for i in range(num):\n",
    "            ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "            ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "            ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Class Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer from a layer of n_nodes1 to a layer of n_nodes2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes of the previous layer\n",
    "    \n",
    "    n_nodes2 : int\n",
    "        Number of nodes of the following layer\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight\n",
    "    \n",
    "    B : ndarray, shape (n_nodes2,)\n",
    "        Bias\n",
    "    \n",
    "    Z : ndarray, shape (batch_size, n_nodes1)\n",
    "        Deepcopy of input\n",
    "    \n",
    "    dW : float\n",
    "        Gradient of weight\n",
    "    \n",
    "    dB : float\n",
    "        Gradient of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize self.W and self.B by using initializer method\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "        self.Z = 0\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.Z = copy.deepcopy(X)\n",
    "        \n",
    "        return np.dot(X, self.W) + self.B\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = np.average(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA) / dA.shape[0]\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Class Initialization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization by Gaussian distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Class Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] Class Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return 1 / (1+np.exp(-self.A))\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        return Z * (1-Z) * dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return np.tanh(self.A)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        return (1-Z**2) * dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray, shape (batch_size, ith n_nodes)\n",
    "        Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        A -= np.max(A)\n",
    "        \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        \n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_samples, 1)\n",
    "            Correct values\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.Z - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 5] Create a Class of ReLU\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "$x$ : a feature (scaler)\n",
    "\n",
    "<br />\n",
    "\n",
    "In addition, the following equation is a differentiation of $f(x)$ with respect to $x$ for backpropagation.\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return np.where(self.A<=0, 0, self.A)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.where(self.A<=0, 0, 1) * dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 6] Initial Value of Weight\n",
    "\n",
    "<br />\n",
    "\n",
    "Xavier's initial value\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "$n$ : Number of nodes of the previous layer\n",
    "\n",
    "<br />\n",
    "\n",
    "He's initial value\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{2}{n}}\n",
    "$$\n",
    "\n",
    "$n$ : Number of nodes of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by Xavier's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by He's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(2/n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 7] Optimization Method\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to implement AdaGrad.\n",
    "\n",
    "$$\n",
    "H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})Ã—E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "$$\n",
    "\n",
    "$H_i$ : Sum of squares of all gradients up to the previous iterations about ith layer\n",
    "\n",
    "$H_i^{\\prime}$ : Updated $H_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    h : float\n",
    "        Sum of squares of all gradients up to the previous iterations about ith layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.h = 0\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.h += layer.dW * layer.dW\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(self.h+1e-7)\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Compute loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def cross_entropy_loss(self, y, y_pred):\n",
    "            \"\"\"\n",
    "            Cross entropy error\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            y : ndarray, shape (n_samples, 1)\n",
    "                Correct values\n",
    "\n",
    "            y_pred : ndarray, shape (n_samples, 1)\n",
    "                Predicted values\n",
    "\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            ndarray, shape (n_samples, 1)\n",
    "                Cross entropy error\n",
    "            \"\"\"\n",
    "            \n",
    "            return np.sum(-1*y*np.log(y_pred+1e-10), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      Train dataset\n",
    "    \n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "      Correct values\n",
    "    \n",
    "    batch_size : int\n",
    "      Size of batch\n",
    "    \n",
    "    seed : int\n",
    "      Seed of random numbers of Numpy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dropout_ratio : float\n",
    "        Ratio of dropout\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    mask : float\n",
    "        Mask\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.mask = None\n",
    "    \n",
    "    \n",
    "    def forward(self, X, train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_ratio\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1-self.dropout_ratio)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return dA * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to validate classes that I created above by 3 layers of neural network with MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform unit8 to float\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform correct labels that are 0 to 9 to \n",
    "\n",
    "# Initialize\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Fit\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "\n",
    "# Transform\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train dataset\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "dnn = ScratchDeepNeuralNetrowkClassifier(100, 10)\n",
    "\n",
    "# 1st layer\n",
    "dnn.add(FC(784, 400, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Dropout())\n",
    "dnn.add(Relu())\n",
    "\n",
    "# 2nd layer\n",
    "dnn.add(FC(400, 200, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Dropout())\n",
    "dnn.add(Relu())\n",
    "\n",
    "# 3rd layer\n",
    "dnn.add(FC(200, 10, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ep loss: 1.68932786349697, val_loss: 1.7879838335254927\n",
      "2ep loss: 1.501297796257158, val_loss: 1.6029079277619895\n",
      "3ep loss: 1.4380480839796466, val_loss: 1.5379981512319327\n",
      "4ep loss: 1.4051821719570081, val_loss: 1.504945739589291\n",
      "5ep loss: 1.3820882812438708, val_loss: 1.4832411194231372\n",
      "6ep loss: 1.3631896053430512, val_loss: 1.467022494159756\n",
      "7ep loss: 1.3464346220956123, val_loss: 1.453865880852911\n",
      "8ep loss: 1.3312154765433202, val_loss: 1.4420715131045463\n",
      "9ep loss: 1.317316062244257, val_loss: 1.4312710749267084\n",
      "10ep loss: 1.304222186705765, val_loss: 1.4211988818215793\n",
      "11ep loss: 1.2919632445545715, val_loss: 1.4112889560050235\n",
      "12ep loss: 1.2805773933228641, val_loss: 1.4013841369718776\n",
      "13ep loss: 1.2701142365539928, val_loss: 1.3917692299657822\n",
      "14ep loss: 1.2603189490259177, val_loss: 1.382678510294578\n",
      "15ep loss: 1.2512147903913926, val_loss: 1.3737798134024999\n",
      "16ep loss: 1.2427478502697713, val_loss: 1.3652654525065802\n",
      "17ep loss: 1.234679499180639, val_loss: 1.3571307198225397\n",
      "18ep loss: 1.227146833205884, val_loss: 1.3491420036836523\n",
      "19ep loss: 1.2201035472542512, val_loss: 1.3413226903698756\n",
      "20ep loss: 1.2135882898138028, val_loss: 1.333884865641812\n",
      "21ep loss: 1.2075379847757177, val_loss: 1.3268715033651426\n",
      "22ep loss: 1.2019719655031058, val_loss: 1.3200471618670981\n",
      "23ep loss: 1.1967089065609042, val_loss: 1.313301588753162\n",
      "24ep loss: 1.1917447998232884, val_loss: 1.306847814743319\n",
      "25ep loss: 1.187090182292312, val_loss: 1.3004137455416607\n",
      "26ep loss: 1.182643738552253, val_loss: 1.294279706473736\n",
      "27ep loss: 1.1783171196420579, val_loss: 1.2882950593061795\n",
      "28ep loss: 1.17427344931818, val_loss: 1.282547980859448\n",
      "29ep loss: 1.1704322524761044, val_loss: 1.2770815403976137\n",
      "30ep loss: 1.1667626161238742, val_loss: 1.2717380598279129\n",
      "31ep loss: 1.1633586200247552, val_loss: 1.2666336993139358\n",
      "32ep loss: 1.1600077313243955, val_loss: 1.2616807215085457\n",
      "33ep loss: 1.1568667572146087, val_loss: 1.2568428364093804\n",
      "34ep loss: 1.153845825667544, val_loss: 1.252248663673936\n",
      "35ep loss: 1.1510568297822104, val_loss: 1.2477319241536093\n",
      "36ep loss: 1.1484407138898438, val_loss: 1.243368025584023\n",
      "37ep loss: 1.1458781321637965, val_loss: 1.239167548100545\n",
      "38ep loss: 1.1434261954011522, val_loss: 1.2351290694753367\n",
      "39ep loss: 1.141033671995017, val_loss: 1.2313024017743208\n",
      "40ep loss: 1.138689728606696, val_loss: 1.2275826280377373\n",
      "41ep loss: 1.1364219633914356, val_loss: 1.2240655760054011\n",
      "42ep loss: 1.13421771982343, val_loss: 1.2205696045055405\n",
      "43ep loss: 1.13202464699545, val_loss: 1.2172309176136684\n",
      "44ep loss: 1.129951639308625, val_loss: 1.213986826721864\n",
      "45ep loss: 1.1279588215377117, val_loss: 1.2107400799309946\n",
      "46ep loss: 1.1259478377348489, val_loss: 1.2076608441619094\n",
      "47ep loss: 1.12404373189526, val_loss: 1.2047021759112728\n",
      "48ep loss: 1.1222024385608782, val_loss: 1.2018211303625193\n",
      "49ep loss: 1.1203304682667237, val_loss: 1.1990361196333919\n",
      "50ep loss: 1.1185232144697197, val_loss: 1.1963173462093155\n",
      "51ep loss: 1.1167341096742232, val_loss: 1.1936696919465453\n",
      "52ep loss: 1.1149476120480724, val_loss: 1.191071038647879\n",
      "53ep loss: 1.1132029044524263, val_loss: 1.1885667867367826\n",
      "54ep loss: 1.1115561199385509, val_loss: 1.1861427451110944\n",
      "55ep loss: 1.109848768081893, val_loss: 1.1838531191072612\n",
      "56ep loss: 1.1082052849942146, val_loss: 1.1815884037824784\n",
      "57ep loss: 1.1066345529231159, val_loss: 1.1794188697025034\n",
      "58ep loss: 1.1050578398910693, val_loss: 1.1773314733802853\n",
      "59ep loss: 1.1035568185085285, val_loss: 1.1752837249288206\n",
      "60ep loss: 1.102080340516681, val_loss: 1.1732665359681291\n",
      "61ep loss: 1.1005765349261944, val_loss: 1.171271743455225\n",
      "62ep loss: 1.0990752501966443, val_loss: 1.169369114460339\n",
      "63ep loss: 1.0975663818545065, val_loss: 1.1675373108604787\n",
      "64ep loss: 1.0961168005691933, val_loss: 1.165745389262026\n",
      "65ep loss: 1.0946673424757318, val_loss: 1.1640608508177515\n",
      "66ep loss: 1.0932771806720223, val_loss: 1.16235879218286\n",
      "67ep loss: 1.0918814802880477, val_loss: 1.1606863380614119\n",
      "68ep loss: 1.0905352710466671, val_loss: 1.1589914582582186\n",
      "69ep loss: 1.0892228344919066, val_loss: 1.1573576132829082\n",
      "70ep loss: 1.0879663480171322, val_loss: 1.155734550555559\n",
      "71ep loss: 1.086673908082931, val_loss: 1.1540970447817671\n",
      "72ep loss: 1.0853593192584898, val_loss: 1.1525001616122594\n",
      "73ep loss: 1.0840703036441837, val_loss: 1.150923976937841\n",
      "74ep loss: 1.08274956390698, val_loss: 1.1493789614411536\n",
      "75ep loss: 1.0814712739372208, val_loss: 1.1479453871162615\n",
      "76ep loss: 1.0802004120852609, val_loss: 1.146526075212392\n",
      "77ep loss: 1.078977944647959, val_loss: 1.145126094136887\n",
      "78ep loss: 1.0777672461376082, val_loss: 1.1437722266574784\n",
      "79ep loss: 1.0765578688574386, val_loss: 1.1423814070030485\n",
      "80ep loss: 1.0754204233092124, val_loss: 1.1409899825897107\n",
      "81ep loss: 1.0742500323945043, val_loss: 1.1396725765717308\n",
      "82ep loss: 1.073124311062294, val_loss: 1.1383343158774493\n",
      "83ep loss: 1.072020496444702, val_loss: 1.1370320282384143\n",
      "84ep loss: 1.0710093467687645, val_loss: 1.1357385813594751\n",
      "85ep loss: 1.0699935747317382, val_loss: 1.1344826596805744\n",
      "86ep loss: 1.0689627180580232, val_loss: 1.1332109236419459\n",
      "87ep loss: 1.0679430972747095, val_loss: 1.1319535831714016\n",
      "88ep loss: 1.066968586724893, val_loss: 1.1306797491037184\n",
      "89ep loss: 1.0659669047246711, val_loss: 1.1294212070637477\n",
      "90ep loss: 1.0649608289396952, val_loss: 1.1281950610528875\n",
      "91ep loss: 1.0639635108696406, val_loss: 1.1269839589071973\n",
      "92ep loss: 1.0629815188968579, val_loss: 1.1257902352506874\n",
      "93ep loss: 1.0619987416683279, val_loss: 1.124570799476396\n",
      "94ep loss: 1.0610395439158617, val_loss: 1.1234000478414887\n",
      "95ep loss: 1.060058908824948, val_loss: 1.1222136362569122\n",
      "96ep loss: 1.0591197649019586, val_loss: 1.1210632705632877\n",
      "97ep loss: 1.0582171009420434, val_loss: 1.119955941070666\n",
      "98ep loss: 1.0572724921360457, val_loss: 1.1188500845134415\n",
      "99ep loss: 1.0563180714006242, val_loss: 1.1177708422158397\n",
      "100ep loss: 1.0554100833074673, val_loss: 1.116683457344917\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "\n",
    "dnn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "y_pred = dnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4FFXbx/HvtvSekEYghdACoYYOAUVAfRBFUaoSQBEBsYHoq4+CPvZeUZRiQUARpKOoRIgi1dA7KaSR3nuy7x8TFiJJCCGbTbL357r22mRnduc+TMgvZ87MGVWmXq9HCCGEANSmLkAIIUTjIaEghBDCQEJBCCGEgYSCEEIIAwkFIYQQBhIKQgghDCQUhADG3HYb3331lanLaFCPhIXxv+efN3UZopGRUBAmFeznR/ivv5q6DNZs3cqEyZPr/XN3hYfjrFbT0s4OH3t7Qtq359tly+p9O0LUF62pCxDC2EpLS9FqTfej7uXtzfG4OPR6Pdu3bmX8qFH06d+ftu3bN8j2Td1+0bRIT0E0Wts2bWJgt260dnJieP/+HD182LDsvddfp1ubNvjY29MnKIiN69YZlq1YvpwRAwbw7BNP4OfiwusLFrBi+XJuHTiQ5+fOxdfZmS7+/mzfutXwnv8MGcLXX35peH9N60ZHRXFbaCg+9vbcecstzJ01i+mTJl2zPSqViuG3346ziwvHrmjL6ZMnuWvYMPxcXAhp3551339vWFZQUMBzTz1FZ19fWjs6cuvAgRQUFACwZcMG+nbqRGsnJ/4zZAinTpwwvC/Yz4/333iD/l264G1rS2lpKYf++YfQHj3wsbdnytixFBUWGtZPS01l7MiRtHZyws/FhdsGDaK8vLxW+0k0LxIKolGKPHiQ2VOn8v7nnxOVlkbYww8zftQoioqKAPBv04atu3YRm5XF/Bdf5OFJk0hKTDS8f/+ePfgFBHA2OZmnnnvO8Frb9u05n5rKY08/zaPTplHdLC81rfvQhAn07N2b82lpPLNgAau/+aZWbSovL2fLhg2kpabiHxgIQF5eHqOHDWPMhAmcTU7my5UreWrmTE4cOwbAf+fOJfLAAX756y+i0tNZ+OabqNVqzp4+zYPjx/Pa++9zLiWF4bffzrg77qC4uNiwvTUrV/L95s3EZGZSXl7OxLvuYuz99xOVns5d997Lhh9/NKz78Tvv4O3jw7mUFM5cvMh/X30VlUpV290lmhEJBdEoff3FF4Q9/DAhffqg0WiYMHkylpaW7Pv7bwDuuvdevLy9UavV3D12LAFt23Jg717D+728vXn40UfRarVYW1sD0MrXl8kPPYRGo2H85MkkJSaSfPFilduvbt0LsbEc3LeP/3vpJSwsLOg3cCC3jRpVY1sSExJo7eSEp7U1k0aP5pV336Vr9+4A/LxpE639/Jg0ZQparZZuPXow6p57WL9mDeXl5Xy7dCmvf/AB3i1botFo6NO/P5aWlqxdvZrh//kPNw0bhk6n49G5cyksKGDPX38ZtvvwnDn4tGqFtbU1+/7+m9KSEmY+/jg6nY47x4yhR69ehnW1Oh1JiYlciIlBp9PRf9AgCQUzJaEgGqULMTF88s47tHZyMjziL1wgKSEBgJVff204tNTayYkTR4+SlppqeH/LVq2u+kwPT0/D1zY2NgDk5eZWuf3q1k1KSMDZxcXwWnXbupKXtzexmZlcyM7m4Tlz2Pn775XauX/Pnkrt/GHFCpKTkkhLTaWwsBD/Nm2u+sykhARa+foavler1bRs1YrE+HjDaz5X1JWUkIBXy5aVftFf+f458+YREBjI6OHD6RoQwHuvv15jm0TzJaEgGqWWrVrx1HPPEZuZaXgk5uczZvx4YmNieOyhh3jr44+JSksjNjOTjp07wxWHgoz1V66HlxcZ6enk5+cbXou/cKFW77W0tGThG29w/MgRNv30E6C0c8DgwZXaGZ+by7uLFuHq5oaVlRVR585d9Vme3t5ciIkxfK/X64m/cAGvli0Nr135b+Dh5UVifHylw2VxsbGGr+3t7XnlnXc4dP48qzZu5JN33+WP336rVbtE8yKhIEyupKSEwsJCw6O0tJTJDz3Ess8+Y/+ePej1evLy8vh582ZycnLIz8tDpVLh1qIFAN8uW8aJo0cbpNbWvr50Dwnh9QULKC4uZu/u3WzbuLHW77ewsGD2U0/x5ksvATBi5EjOnj7Nqm++oaSkhJKSEg7u28epEydQq9VMmjqV5558ksSEBMrKyti7ezdFRUWMvu8+ftm8mT9++42SkhI+fucdLCwt6dO/f5Xb7d2vH1qtls8+/JDS0lI2rF1b6XDbtk2bOH/2LHq9HnsHBzQaDWqN5sb+sUSTJKEgTO7e22/H09ra8Hh9wQK6h4TwwRdfMG/2bHydnekRGMh3y5cD0CEoiNlPPcWwfv1o6+HB8SNH6DNgQIPV+8WKFezbvZsAV1f+9/zzjB47FgtLy1q/f9LUqcTFxrJ140bs7e1Z98svrF21ig7e3rTz9OTF+fMNA+ovv/02QcHB3NyrF/4uLrw4fz7l5eW0bd+ez7/9lqcffZQ2bm5s3biRVRs3YmFhUeU2LSws+GbtWr5bvhw/Z2fWrV7NHXffbVh+7swZ7rzlFlra2TG8Xz+mzZzJoCFDbujfSTRNKrnJjhA3ZsrYsbTt0IH/W7jQ1KUIccOkpyDEdTq4bx9R585RXl7Or9u2sWX9ev5z112mLkuIeiGXOQpxnS4mJXH/3XeTnpaGt48P7yxaZDjFVIimTg4fCSGEMJDDR0IIIQya3OGjNm5u+Pn51em9eXl52Nra1m9BTYA5ttsc2wzm2W5zbDNcf7ujoqM5f8UFntVpcqHg5+fH/v376/Te8PBwhpjhaXbm2G5zbDOYZ7vNsc1w/e3uHhJSq/Xk8JEQQggDCQUhhBAGRguFWVOnEujuTr/OnatcnpWVxdg77mBA16707dRJ7kYlhBCNgNHGFCaEhfHQ7Nk88sADVS7/8pNP6BAUxOqNG0lNSSGkfXvumzix2sv0hRDmraSkhLi4OAqvuDkQgKOjIyeuuMGQuaiu3VZWVvj4+KDT6er0uUYLhQGhocRER1e7XKVSkZuTg16vJzc3F2cXF7lloBCiWnFxcdjb2+Pn51dpBticnBzs7e1NWJlpVNVuvV5PWloacXFx+Pv71+lzTfZb+KHZsxk/ahQdvL3Jzclh6erVqNVVH81avngxyxcvBiAxLo7w8PA6bTM3N7fO723KzLHd5thmaN7tdnR0xNXVldx/3QOjrKyMnJwcE1VlOtW128LCgszMzDr/HJgsFH7/+WeCu3Vj4++/E3XuHHcNG0a/QYNwcHC4at2w6dMJmz4dgKEhIXU+/UxOXTMf5thmaN7tPnHiRJW/H6SncDUrKyu613HqFZOdfbRi2TLuuPtuVCoVAYGB+Pr7c+bkSeNt8OJx/M9/C3lpxtuGEEI0cSYLBZ/WrQ13dkq+eJGzp07hFxBgvA2mncU39gfISTDeNoQQzZqdnZ2pSzA6ox0+mjZ+PBHh4aSlphLk48MzCxdSWlICwNQZM5j33/8yMyyM/sHB6PV6FrzxBq5ubsYqB6wclefCLONtQwghmjijhcKSlStrXO7l7c26X34x1uavJqEghKgner2ep59+mq1bt6JSqXj++ecZO3YsiYmJjB07luzsbEpLS1m0aBH9+/dn2rRp7N+/H5VKxdSpU3niiSdM3YRqmc85oIZQyDZtHUKIG7Zw4zGOJyj/l8vKytDUw/2kg7wdePGOTrVad+3atURGRnLo0CFSU1Pp1asXoaGhfPfdd4wYMYLnnnuOsrIy8vPziYyMJD4+nqMV9xHPzMy84VqNyXymuZCeghCinkRERDB+/Hg0Gg0eHh4MHjyYffv20atXL5YtW8aCBQs4cuQI9vb2BAQEcP78eR599FG2bdtW5RlUjYn59BQsK3aEhIIQTd6Vf9Gb4pTU6u5NFhoays6dO9m8eTP3338/8+bN44EHHuDQoUP8/PPPfPLJJ3z//fcsXbq0Qeu9HubTU9BoKdVYSSgIIW5YaGgoq1evpqysjJSUFHbu3Env3r2JiYnB3d2dhx56iGnTpnHw4EFSU1MpLy/nnnvu4eWXX+bgwYOmLr9G5tNTAEq1tmiLJBSEEDdm9OjR7N69m65du6JSqXjzzTfx9PTkq6++4q233kKn02FnZ8fXX39NfHw8U6ZMoby8HIDXXnvNxNXXzKxCoUxjKz0FIUSdXZpiQ6VS8dZbb/HWW29VWj558mQmT5581fsae+/gSuZz+AilpyChIIQQ1ZNQEEIIYSChIIQQwsDMQsFGLl4TQogamFkoVPQUqjnHWAghzJ35hYK+DIrzTF2KEEI0SmYWChXT3sq4ghBCVMnMQsFG+aJIxhWEEMZV070XoqOj6dy5cwNWU3tmFgq2yhfSUxBCiCqZ1RXNEgpCNBNbn4GkIwBYl5WCph5+lXkGw22vV7t4/vz5+Pr6MnPmTAAWLFiASqVi586dZGRkUFJSwv/+9z/uvPPO69psYWEhjzzyCPv370er1fLuu+9y0003cezYMaZMmUJxcTHl5eX8+OOPeHt7c9999xEXF0dJSQkvvvgiY8eOvaFm/5uEghBC1MK4ceN4/PHHDaHw/fffs23bNp544gkcHBxITU2lb9++jBo1CpVKVevP/eSTTwA4cuQIJ0+eZPjw4Zw+fZrPPvuMxx57jIkTJ1JcXExZWRlbtmzB29ubzZs3k5OTY5hPqT5JKAghmp4r/qIvaKCps7t3705ycjIJCQmkpKTg7OyMl5cXTzzxBDt37kStVhMfH8/Fixfx9PSs9edGRETw6KOPAtChQwd8fX05ffo0/fr145VXXiEuLo67776btm3bEhwczNy5c5k/fz4333wzI0aMqPd2Gm1MYdbUqQS6u9OvmsGUD996i4HdujGwWzf6de6Mi0ZDRnq6scoBoExTMdAsoSCEqIMxY8awZs0aVq9ezbhx41ixYgUpKSkcOHCAyMhIPDw8KCwsvK7PrO7eDBMmTGDDhg1YW1szYsQIfv/9d9q1a8eBAwcIDg5mwYIFvPTSS/XRrEqMFgoTwsJYs21btcvnzJtHRGQkEZGRvPDaawwYPBhnFxdjlQNAucYCtHJPBSFE3YwbN45Vq1axZs0axowZQ1ZWFu7u7uh0Onbs2EFMTMx1f2ZoaCgrVqwA4PTp08TGxtK+fXvOnz9PQEAAc+bMYdSoURw+fJiEhARsbGyYNGkSc+bMMcrsq0Y7fDQgNJSY6OharfvjypWMGT/eWKVUZuUooSCEqJNOnTqRk5NDy5Yt8fLyYuLEidxxxx2EhITQrVs3OnTocN2fOXPmTGbMmEFwcDBarZbly5djaWnJ6tWr+fbbb9HpdHh6evLCCy+wb98+5s2bh1qtRq1Ws3jx4npvoyqzur5LPYiJjmbcyJHsrrhhdVXy8/MJ8vHhn7Nnq+0pLF+8mOUVjU+Mi2PVqlV1qic3N5ebjz1Nrp0fxzs9XafPaIpyc3NrPGe6OTLHNkPzbrejoyOBgYFXvV5WVoZGozFBRaZVU7vPnj1LVlblP36fmDuX8P37r/m5Jh9o3rZxI30GDKjx0FHY9OmETZ8OwNCQEIYMGVKnbYWHh2Pj4oWNlSXudfyMpig8PLzO/2ZNlTm2GZp3u0+cOFHlgLIp7tHcGNTUbisrK7p3716nzzV5KPy4alWDHDr69fhF5u7IZ7evHdZy+EgI0QCOHDnC/fffX+k1S0tL9uzZY6KKrs2koZCVlcWff/zB4m+/Nfq2VCrILNJTqLHHOi/O6NsTQtQ/vV5/XdcAmFpwcDCRkZENus0bHREwWihMGz+eiPBw0lJTCfLx4ZmFCyktKQFg6owZAGxat46bhw/H1tbWWGUYOFjrACjU2Mo9FYRogqysrEhLS8PV1bVJBUND0uv1pKWlYWVlVefPMFooLFm58prrTAwLY2JYmLFKqMTeSmlqvtpOzj4Sogny8fEhLi6OlJSUSq8XFhbe0C/Bpqq6dltZWeHj41PnzzX5mEJDcbBSegp5KlsoK4KSQtCZ3w+SEE2VTqfD39//qtfDw8PrPKjalBmr3WYzS+qlnkK2Xq5qFkKI6phNKNhaaFEB2Xpr5QW5p4IQQlzFbEJBrVZhrYWMcukpCCFEdcwmFABsdCrSSit6CoWZpi1GCCEaIbMKBWutitTSisFl6SkIIcRVzCoUbLRwscRS+UauVRBCiKuYVyjoVFwslp6CEEJUx7xCQasipUANaq2EghBCVMGsQsFaCzlFpWDpIKEghBBVMKtQsNGpyCkqRW/lKNcpCCFEFcwqFKy1KvR6KJeeghBCVMmsQsFGmf6IEp2EghBCVMW8QkGrTLdbopWZUoUQoipmGQqFGnu5TkEIIapgVqFQcZ8d8tW20lMQQogqmFUoXOop5KltoSQPykpMXJEQQjQuZhkKuVyaKVUOIQkhxJWMFgqzpk4l0N2dfp07V7vOrvBwBnbrRt9Onbh98GBjlWJw6fBRluFGOzJTqhBCXMlooTAhLIw127ZVuzwzM5O5M2eycsMG/j52jK9++MFYpRjo1CostWoyL91TQS5gE0KISowWCgNCQ3F2cal2+ZrvvuOOu++mVevWALRwdzdWKZU4WOtI1jsq32TFN8g2hRCiqdCaasNnT5+mtKSE/wwZQm5ODjMee4zxDzxQ5brLFy9m+eLFACTGxREeHl6nbebm5qIpV7MnxZJZQNSeLcRctKtjC5qO3NzcOv+bNVXm2GYwz3abY5vBeO02WSiUlZYSeeAA63/7jcKCAob160evvn0JbNfuqnXDpk8nbPp0AIaGhDBkyJA6bTM8PBxPFx16Ky2ofPG3zce/jp/VlISHh9f536ypMsc2g3m22xzbDMZrt8lCwdvHBxc3N2xtbbG1taV/aChHDx2qMhTqk72VlpzCUvDoBBePG3VbQgjR1JjslNTb77yT3bt2UVpaSn5+Pgf27KFdx45G366DtY7swhJwD4K0s1BaZPRtCiFEU2G0nsK08eOJCA8nLTWVIB8fnlm4kNIS5WKxqTNm0L5jR2659VYGdOmCWq3m/gcfJKiG01fri4OhpxAE+jJIOQVeXYy+XSGEaAqMFgpLVq685jpz5s1jzrx5xiqhSg5WOrILSsC9k/JC8nEJBSGEqGBWVzSDMqZQVFpOkaMfaCzg4jFTlySEEI2G2YWCQ8VlzTklKnBrr/QUhBBCAGYYCvZWyhEzw7iCnIEkhBAGZhcKDlZKT0EZVwiCnAQoyDBxVUII0TiYXSjYV4SC4VoFkN6CEEJUMLtQcLBWDh8ZrlUAGVcQQogKZhcKl3sKJeDgDVaOcgaSEEJUMLtQcKgYaM4uKAWVSuktSE9BCCEAMwwFWwstKlVFTwEqQuEE6PWmLUwIIRoBswsFtVqFvaWW7MJS5QWPIOVmO1lxpi1MCCEaAbMLBVDGFbIv9RQ8KuZbSow0XUFCCNFImGUoOFjrlDEFAO8eYOUEJzaatighhGgEzDIUlHsqVPQUtBYQNApOboaSAtMWJoQQJmaWoeBgpbs8pgDQ+R4ozoUzv5iuKCGEaATMNBSu6CkA+A0CW3c4+qPpihJCiEbAPEPBuuKeCpeoNdBpNJz+GQqzTVeYEEKYmFmGgr2VlpyiUsrLr7g2ofM9UFoIp7aarjAhhDAxswwFBysdej3kFV8xruDTCxxbySEkIYRZM1oozJo6lUB3d/pVc9/lXeHhtHZ0ZGC3bgzs1o03XnrJWKVc5dI9FSoNNqvVyiGkc79BfnqD1SKEEI2J0UJhQlgYa7Ztq3GdfoMGEREZSURkJPNfeMFYpVzFcPe1KwebAYLHQHkpHFrVYLUIIURjYrRQGBAairOLi7E+/oY4VoRCel5x5QWeXZQzkSLeheI8E1QmhBCmpTXlxvfu3s2Arl3x8vbm5bffpmOnTlWut3zxYpYvXgxAYlwc4eHhddpebm4u4eHhpBeWA7D1z38ovqCrtI6D80h6RO/i/Mr5xPqOqdN2GptL7TYn5thmMM92m2ObwXjtNlkodO3RgyMxMdjZ2fHLli1MvOsuDp45U+W6YdOnEzZ9OgBDQ0IYMmRInbYZHh7OkCFD0Ov1LNyznTJ7T4YMCf7XWkMgL5yA2A0E3PcyWDvXaVuNyaV2mxNzbDOYZ7vNsc1gvHab7OwjBwcH7OzsABh+++2UlJSQlpraINtWqVR08LTnZGI11yTc/LxyvcKfHzZIPUII0ViYLBQuJiWhr7iHwYG9e9GXl+Pi6tpg2+/o5cCppJzK1ypc4tlZGXTe8xnkXGywmoQQwtSMdvho2vjxRISHk5aaSpCPD88sXEhpiXK2z9QZM1i/Zg1LFy1Co9VibW3NklWrUKlUxirnKh087ckrLiMuo4DWrjZXrzDkWTi2Dn5+FsYsbbC6hBDClIwWCktWrqxx+fTZs5k+e7axNn9NHbwcADiemF11KLi2gcHPwI7/QbvboMu9DVyhEEI0PLO8ohmgnYcdKhWcTKphrqOBT0CrPrD5KciMbbjihBDCRMw2FGwstPi72nIyMaf6lTRaGP056Mtg3SNQXtZwBQohhAmYbSgAdPCyr7mnAODiD7e9CTERsPPthilMCCFMxLxDwdOBmPR88opKa16x2wToMg7CX4XDPzRMcUIIYQJmHgr26PVw+mINh5AAVCoY9SH4DoT1MyHmr4YpUAghGphZh0LHijOQTiZdIxQAtJYw7ltw9oOV4yG16quvhRCiKTPrUGjpZI2dpbb6K5v/zdoZJv4AGh18fRdkxBi3QCGEaGC1CoWoc+coKioClPsgfPbhh2RmZhq1sIagVqto72nPidr0FC5x9oNJa6E4B74eBdkJRqtPCCEaWq1C4f577kGj0XD+7FkenTaNmKgoHpowwdi1NYhLcyBdmnKjVry6wP3rIC8NvhoFucnGK1AIIRpQrUJBrVaj1WrZtG4djzz+OK+99x5JiYnGrq1BdPByILuwlISswut7Y8ueyqGk7HhYPlJ6DEKIZqFWoaDT6VizciUrv/qKESNHAhjmMWrqgrzsATgSV4fDYb79YNKPSiAsvRXSo+q5OiGEaFi1CoVPli1j7+7dPPXcc/j5+xMdFcV9kyYZu7YG0cXHCWcbHVuOJNXtA3z7w+T1UJQNy26DlFP1W6AQQjSgWoVCh6Ag3vzwQ8aMH09mRga5OTk88cwzxq6tQeg0am4L9mL78YvkF1/jIrbqtOwJYVuUaTCWjoDYPfVbpBBCNJBahcJ/hgwhOzubjPR0BnbtyqwpU/i/J580dm0NZlRXbwpKyvjtxA0MGHsEwbSfwdpFOSvpxMb6K1AIIRpIrUIhOysLBwcHNq5dy4QpU/jjwAHCf/3V2LU1mF5+Lng4WLLh0A0OFrsEwLTt4BkMq++HPYvrp0AhhGggtQqFstJSkhITWff999xaMdDcnGjUKkZ28eaPUylkFdzgALqtKzywAdrfDlvnwc/PQXl5/RQqhBBGVqtQePqFF7h7xAj827ShR69eRJ8/T5u2bY1dW4Ma1dWb4rJyfj5axwHnK1nYwNhvoM8M2P0x/DAZSgpu/HOFEMLIahUKd917L38dPsy7ixYB4BcQwDc//mjUwhpaFx9HfF1tbvwQ0iVqDdz2Box4TRlf+OoOyEutn88WQggjqVUoxMfFMXH0aALd3Wnr4cH999xDfFxcje+ZNXUqge7u9Ovcucb1Du7bh4tGw/o1a2pftRGoVCru6OLNX+dSSc65zgvZatJvptJrSDoCX94CqWfr77OFEKKe1SoUZk2Zwm2jRnEyIYET8fHcescdzJoypcb3TAgLY822bTWuU1ZWxovz5zN0xIjaV2xEd3bzplwPaw7UHHjXreMdELYZinJgyS0Qs7t+P18IIepJrUIhNSWFSVOmoNVq0Wq1TAwLIzUlpcb3DAgNxdnFpcZ1Pv/oI0bdcw9u7u61r9iI2nrYM6itG0t2RdX9moXq+ITAg7+CjZtyyurh7+v384UQoh5oa7OSq5sbq7/9ljHjxwOwZuVKXFxdb2jDCfHxbFq3jo2//87BfftqXHf54sUsX6yc3pkYF0d4eHidtpmbm3vN94a6lLHrTDEvrdjBrf66Om2nJtoOL9Lp2Bs4r32I6APbifYbDyrjzmBem3Y3N+bYZjDPdptjm8GI7c7U6/XXehyJidHfescdelc3N71bixb62++8U38kJuaa7zsUFaXv2KlTlcvuHDNGv333bn2mXq8fP3my/qsffrjm52Xq9fqePXvq62rHjh21Wm/CF7v1PV/ers8vKq3ztmpUUqTX/zRLr3/RQa9f/YBeX5RrnO1UqG27mxNzbLNeb57tNsc26/XX3+5uPXvW6ndsrf5EbdW6Nas2bOBcSgpnk5P57qef2Lh27Q2F0T/79zN13DiC/fzYsGYNT82cyaaffrqhz6wvc25uS2puESv3xhpnA1oLGPURDHsZjq+HJcNlMj0hRKNQ5+MWn7777g1t+HBUFEeiozkSHc2oMWN459NPGXnXXTf0mfWlT4ArfQNc+OyPcxSWlBlnIyoVDJgDk9ZAVhwsHgJnfzPOtoQQopbqHAr6a9yUZtr48Qzv148zp04R5OPD10uWsPSzz1j62Wd13WSDmjO0Lck5RXz7t5FvuRl4C0zfAQ4t4dt7YOfbcgW0EMJkajXQXBWVSlXj8iUrV9b6sxYtX17XMoymX4Aroe1a8MGvZxjVzRt3eyvjbcwlAB7cDhvmwO8vQ/xBGL0IrByNt00hhKhCjT0FH3t7Wjk4XPXwsbcnMaF532lMpVKxcFQnikrLeW3LSeNv0MIW7vkSbn0DzvysHE5KOmL87QohxBVqDIW4nBwuZGdf9YjLySGttJ7P42+E/N1smR4awLp/4tlzPs34G1SpoO8MmLxJmSvpi6Gwfylcz/2jhRDiBhj3BPlmYNZNgbR0suaF9ccoKWugY/2+/WBGBPgNhE1PwJqpUJjVMNsWQpg1CYVrsLbQ8OIdQZy6mMPSiAY8bdTWDSaugaEvKKetfjZQ7ugmhDA6CYVaGBbkwbAgD97ZfpqzyTkNt2G1GgY9BVO2AipYdivseA3Kmv+hOyGEaUgo1IJKpeLV0cE8ZuVbAAAgAElEQVTYWmh48vtDDXcY6ZLWfZTDScH3wR+vK+GQfr5haxBCmAUJhVpqYW/JK6ODORyXxaLwcw1fgJUD3P053LMEUk/DooFw8GsZhBZC1CsJhetwe7AXo7p68+FvZzgab6KB3+Ax8Mhf4NMTNjwKqyZAbrJpahFCNDsSCtfppTs74WJrwWOr/iGvyETH9h194P71MPwVZWqMT/sqg9FCCHGDJBSuk5ONBe+P60ZUah7P/3T0mtN9GI1aDf1nw8M7wbEVfP8ArJkGeQ1wPYUQotmSUKiD/m3cmDO0Lev+ieeH/fV8l7br5d5BuXnPkGeV3sInveHojzLWIISoEwmFOnr05rb0b+PKf9cf5WRStmmL0ehgyDNKr8GptXKx28pxkGHkyfyEEM2OhEIdadQq3h/XDXsrHY98e5Cs/BJTlwQeQTBtOwz/H0Ttgk/6QMR7qMrlugYhRO1IKNwAd3srFk3qQVxGPrNXHqS0oa9fqIpGC/0fhVl7IHAo/LqAkP2PQ9ROU1cmhGgCJBRuUC8/F16+szO7zqTy+tYGmE21tpxawbgVMH4V6vJi+OoO+GEKZDfv2W2FEDemzvdTEJeN692ak0k5fBkRRXtPe+4NaWXqki5rfxv7eqkJ1fwDEe/B6Z9h8DzoOxO0lqauTgjRyEhPoZ48/5+ODAx04//WHeGvc6mmLqeSco2lMhA9aw8EDIFfF8Cn/eDMdhNXJoRobCQU6olWo+aTiT3wc7Xl4W8OcOZiA06cV1vOfjD+O5j4o3LvhhVjlFuAJp8wdWVCiEbCaKEwa+pUAt3d6de5c5XLN69fT/8uXRjYrRtDQkLYHRFhrFIajKO1jmVTemGl0xC2bB/JOYWmLqlqbW+BR3bDiFfhwj5YNAA2PQl5jauHI4RoeEYLhQlhYazZtq3a5YOHDuXPQ4eIiIzk46VLmfPgg8YqpUH5ONuwdHIv0vOKmbp8HzmFjeBU1apoLaDfLJjzD4RMhQPL4cPu8OcHUFpk6uqEECZitFAYEBqKs4tLtcvt7OxQqVQA5OflGb5uDoJ9HPl0Ug9OJubw4Ff7KSwpM3VJ1bN1hf+8rUyy17ovbH8BPu4Fh3+A8kZwiq0QokGpMo04eU9MdDTjRo5k99GjVS7fuG4dLz37LCnJyXy/eTO9+/Wrcr3lixezfPFiABLj4li1alWd6snNzcXOzq5O762LvxNK+fxwEV1baJjd3RKt2jTBdz3tdk7/hzbnlmOXF02OXQDnAx4gw7mbMgbRhDT0vm4szLHd5thmuP52PzF3LuH79197xUy9Xm+sx6GoKH3HTp2uud7mP/7QDx46tFaf2bNnT31d7dixo87vrauvd0frfedv0j+28qC+tKy8wbev19eh3WVlen3kKr3+vc56/YsOev2y/+j1sXuNUpuxmGJfNwbm2G5zbLNef/3t7tazZ61+xzaKs48GhIYSde4caanNb6Dz/r6+zBvRnp8iE5i35hBl5U1gojq1GrqOhdn74bY3IeUkLLkFvhsHiYdMXZ0QwohMFgrnz541TDsdefAgJcXFuLi6mqoco5p1UyBPDmvH2oPxTScYQLm4rc/DMCcShr4AsX/B56GwaiIkHTF1dUIIIzDaFc3Txo8nIjyctNRUgnx8eGbhQkpLlDNxps6YwYYff2TV11+j1emwtrZm6erVzWqw+d/mDG0LwLvbT4Me3hzTBa2mUXTUrs3SDgY9BSHTYM9nsPtTOLkJ2t+uvO4TYuoKhRD1xGihsGTlyhqXPz5/Po/Pn2+szTdKc4a2Ra2Ct385TW5RKR+O746VTmPqsmrP2km5MrrPw7Dnc/h7EZzaAv6DlXDwD21yA9JCiMqayJ+qzcfsm9uycFQntp+4yOSle8lurNcx1MTaWQmHJ47CsJeVMYevR8EXN8PxDVDeiE/BFULUSELBBCb39+P9sd04EJPBuM//5mJ2I73y+Vos7WHAHHjsMIx8Hwoy4Pv74eMQ2PsFFOeZukIhxHWSUDCRO7u15MvJIcSk5XHXJ39yItHEd2+7ETorCJkCjx6Ae5eDtQtsmQvvBsH2FyHLxLcsFULUmoSCCQ1p7873M/qh18OYRX8RfirZ1CXdGLUGOo2Gh36Dqb8oYwx/fQjvd4EfwiD2b7l3tBCNnISCiXXydmTdrP74utoydfk+vtx13nCqbpPWug+M/QYeO6TMsXTud1g6Qjml9Z9voaTA1BUKIaogodAIeDla88OMfgwP8uR/m0/w+OpICoqbyWCtU2sY/jI8eQJGvgdlJbB+FrzTAbY+AymnTF2hEOIKEgqNhK2llkWTejBvRHs2HErgnkV/EZ3ajAZqLWyV2Vhn7obJm6DNzbDvS/ikNyy9DQ6tlt6DEI2AhEIjolKpmHVTIEsn9yIuI5+RH0Ww6XAzu6eySgX+g+DeZUrv4ZaFkJsE66bDO+1hy9OQeNjUVQphtiQUGqGbOriz5bFBtPOwY/Z3//B/6440n8NJV7JrAQMfh9kHYPJGCBym3Nfh80Hw2SD4+zO58Y8QDUxCoZHycbZh9cP9mDG4Dd/tieWOjyM4lpBl6rKMQ61WzlQaswSeOgm3v628vm2+0ntYOUG5KK6kiV7PIUQTIqHQiOk0ap65rQPfTOtNdkEJd33yJ4t3nqO8qUyoVxc2LtD7IZixC2b8CX1mQPx+5aK4t9spg9Tnw6Gs1NSVCtEsSSg0AYPatmDb46Hc3MGdV7ecZNziv4lqToPQ1fHsDCNegSeOw6S10OF2OPYTfH0nvNsBNj0BUTslIISoRxIKTYSLrQWfTerJW2O6cDIpm9s+2MmXu843nWm4b4RGC4FDYfRnMPcM3Pc1+A2EQ6vgqzvgnXawYQ6c/RVKi01drRBNmtFmSRX1T6VScW9IK0LbteC5dUf43+YTbDiUwKujg+nc0tHU5TUMCxsIulN5FOfBme1wYgMc/REOfgWWDhB4C+7l/lDQTZnZVQhRaxIKTZCHgxVfPBDCxsOJvLTxOKM+juCBfn48Nbwd9lY6U5fXcCxsodNdyqOkUBlrOLUZTm0lKG8tnPpA6VG0vx3aDgcXf1NXLESjJ6HQRKlUKkZ19WZwuxa8/fMpvtodzabDCTw1vD33hbRCozaz+xrorKD9rcqjvIyDGxfTwyZJud/D1qeVh1s7JRzaDofW/UBrYeqqhWh0ZEyhiXO01vHyXZ3ZMGsgfq62PLv2CCM/iuDPs2Z8fr9aQ7ZjRxi2EGbvg0cPwq2vg0NL2LtYuffDm/7Kqa77l0LmBVNXLESjIT2FZiLYx5EfZvRj85FEXttykolf7mFgoBtP39re1KWZnmsbcH0E+j4CRbnKGUtnt8OZX5XDTQAtOihTbwTcBH4DlENTQpgho4XCrKlT+XnTJlq4u7P76NGrln+/YgXvv/EGAHZ2dryzaBHBXbsaqxyzoFKpGNnFm1s6erBiTyyf7DjLqI//pKeHBre2WeYzGF0TSzvl1NYOtyvTeKeeVgarz26HfUvg709BrYNWvZXbjPqHQsuecqhJmA2jhcKEsDAemj2bRx54oMrlvv7+bPnjD5ycndm+dSuPT5/Ob3v2GKscs2Kl0zBtoD/3hfiwJCKKz8PPMPKjCG7u4M7smwPp0drZ1CU2DioVtGivPPrPVibki90N53ZA1B8Q/hqEvwo6G2jdF/wGKQ/vbqAxowF9YVaMFgoDQkOJiY6udnmf/v0NX/fq25eEOLk7V32zt9Lx+C3taKeP57zGhy8jorj707/o7efCw4MDuKm9O2pzG5Cuic5aOYTU5mbl+/x0iI6A6F3K828LK9azUXoSrfuDbz9oGaKcKitEM6DKNOIdXWKioxk3cmSVh4+u9NHbb3P65Ek++vLLKpcvX7yY5YsXA5AYF8eqVavqVE9ubi52dnZ1em9TdqndhaV6dsaV8nN0CWmFerxtVQzz1dG/pRZLTfMKB2Psa11xFo5Zx3DKPIpT5jFs82JQoadcpSXHvg1Zjh3JduhIlmNHSixMc6jOHH/GzbHNcP3tfmLuXML377/meiYPhZ07djB35ky2RUTg4up6zc8cGhLC/lo0rCrh4eEMGTKkTu9tyv7d7pKycjYdTuDLXVEcS8jGyUbH2F6tmNjbl9auzeMv3gbZ1wUZcGEvxPyl3Go04SCUVVxR7dIGWvVRehSteisD2WqNcevBPH/GzbHNcP3t7h4SUqtQMOnZR0cPH2bOgw+yZuvWWgWCqB86jZrR3X24q1tL9kVnsDQiii92nmfxzvOEtm3BhD6tubmDOzqNnLFcI2tnaDdCeYByAV1ipBIQF/bCmZ/h0HfKMgs7aNlDOdTUsqfycPAyXe1CVMNkoXAhNpb7776bz7/5hsB27UxVhllTqVT09neht78LiVkFrNp7gVX7Ynn4mwO0sLdkTE8f7gtphb+bnJ5ZKzorZUC6dV/le70e0s9D3H6I26c8/voQyism8LPzBO/uysC1Vzfl2d7TdPULgRFDYdr48USEh5OWmkqQjw/PLFxIaUkJAFNnzODNl14iPS2Np2bOVArRamvVtRHG4eVozRPD2vHozYHsOJXC6n2xfP7HORaFn6O3nwtjQnz4T7AXtpZyaUutqVQV10i0ga5jlddKCiDpCMQfgIRIpWdxehtQcRTXzgM8u4BXF+XZMxic/ZV7TgjRAIz2P3zJypU1Lv/oyy+rHVgWpqPVqBkW5MGwIA+Ssgr58WAcaw7E8fSawyzYcIzhQR7c2b0lAwPd5PBSXeisL48zXFKUqwRFYqQSFElH4NzvoK+4257OFjyCwKMzeHRSnt07ymR/wijkzz5RLU9HK2bdFMjMIW04EJPBjwfj2Hw4kZ8iE3C1teDWzp6M7OJNb38X85trqT5Z2imntvr2u/xaSSEkH4eLRyHpqPJ8bC0cWHZ5HQcfJSzcg5SwcA9CVV7S8PWLZkVCQVyTSqUixM+FED8XFozqxB+nUlh/KIG1B+NZsScWNztLRnTy4LbOXvQJcJEeRH3QWVUMTPe4/JpeD9kJSkBcPFYRGseVi+0qwiAUNRwPVM52atFBuTDPrR24tVV6KUJcg4SCuC6WWg3DO3kyvJMn+cWl/H4ymS1HEg0B4Wit46b2LRja0YPB7VvgYE5TeRubSgWOLZXHpTOeQLmxUNpZSD5O7L5t+NoUKKFxchPoyy+9GZxaXw4I1zbgGqg87L1lzEIYSCiIOrOx0DKyizcju3hTWFLGH6dT+PlYEjtOJvNTZAJatYoQP2du7uDOzR08aNPCFpVKDjPVO61FxZhDEFFpbvheOne9pBDSz0HKKUg9A6mnIOW0cnV2acEV77e+PCDuGqhcY3EpMGxclDASZkNCQdQLK52GEZ08GdHJk7JyPZEXMvj1RDI7Tibz6paTvLrlJD7O1gxq24LB7dzoF+CGo430IoxKZ1UxMN2p8uvl5ZCTqARG6hlIO6f0NJKOwIlNlwe4AaycKoIiQAkNlzbgGqB8by1zaDVHEgqi3mnUKnr6utDT14X5t3YgPrOAHSeT2Xk6hY2HEli5Nxa1CoJ9nBgY6Eq/ADd6+DphYyE/jg1Crb58GMo/tPKyshLIjFVC4lJYpJ1RJgo88gOGU2cBrF0qh4VLRVi4SmA0ZfK/UBhdSydrJvX1ZVJfX0rKyvknNpOIs6n8dTaVz/44zyc7zqFVqwj2caS3nws9fZ3p6euMq52lqUs3Pxrd5UNJ/1ZSCBlRygV5aeeUnkbaOYj+Ew6vrryulZNy+1OXAOU6C2c/5XsnX3DwbpApP0TdSCiIBqXTqA1XUT85rB25RaXsj05nb1Q6e6LSWfZnNJ/vPA+Av5stPVorAdG9tRPtPOzl1FdT0lkp10e4d7x6WUkBZERDepQSFulRSoDEH4BjP1U+JKXWgqOPMvDt1FoJCsdWFV+3Uga+NfKryVTkX16YlJ2lliHt3RnS3h2AwpIyjsRnsT86gwMxGYSfSubHg8q06rYWGoJ9HOnayoluPk50beWEl6OVDF43Bjrr6gOjrASy4pSQyIhRDk9lxii3QT3zK+QmVV5fpVF6E44+l8PDsZUSGI4VwSGn1xqNhIJoVKx0Gnr5udDLzwUAvV5PTFo+/1zIIDI2k8gLmSyNiKKkTDm23cLeki4tHeni40SXVo4Et3TETQ47NS4aXcWhJP+ql5cUQnZ8RVDEKgGSeUF5vrAHjq27PF/UJbYtlHtuO/oQmKMCi6MVIdJKebZtIafZ1pGEgmjUVCoVfm62+LnZMrq7DwBFpWWcSMzh0IVMDl3I5HB8Fr+fSubSJPCeDlZ4WZUQWXqaTt6OdG7pgKeD9CgaLZ1V9eMYAOVlytlSmbEVYRFbER7xkHoGr/QYiN9Y+T1q3eXehkPFoLrDpUfF6zaucrptFSQURJNjqdXQrZUT3Vpdnvsnt6iUo/FZHI3P4kh8FnvOJPLBb2cMQeFia0Enbwc6eTvS0cueIC8H/N1s0crV142fWnP5UJLv1Yt37djBkL7dlJ7FlY/seCU4LvwNxxKu7m1oLJXpy68Mi38HiK2b2QWHhIJoFuwstfQNcKVvgHJfjvDwLHr1G8jJpGyOxmdzLCGLYwnZLIk4bzj0ZKFVE9jCjg6e9rSveHTwdMDDwVJ6FU2JSqWcAmvtrMwqW5XycshLUYIiO16ZLuRScGQnKMGRnWiYLsRAY3E5LBy8wf5SiHgpA+IOXsrMts3ont0SCqLZsrXUGq6XuKS4tJxzKbmcTMrmRGIOJ5Ny+OtcGmv/iTes42Sjo527PW097GjnYU+gux1t3e1oYS9h0WSp1WDvoTyunE/qSjUGR6JyP4zshMt31zNQgZ17RWBcCo6K0LD3rHjNUzlNtwn8/EgoCLNioVXT0cuBjl4OjO5++fXM/GJOJeVw6mIOJxJzOHMxh42HEsguvHzIwd5KS1t3O9pWBEagu/LwdrRGLafKNn21CQ69HvLTlHDISbz6OSNGudCvIOPq92qtK3oWnkpI2Hsp27LzvPxs5670eEwYHhIKQgBONhb0CXClT8Dl28Lq9XqSc4o4m5zL2eRcziTncOZiLr+euMjq/RcM61np1AS4KQHRpoUdbdxtadPCDn83W6x0cpFWs6JSKeMMtm7KjZCqU1IAOUlXBMYVX+deVO6dcWpr5TmoLtFYKoek7NyV8LBzr/i+Iqy8uhqvfUgoCFEtlUqFh4MVHg5WDAh0q7QsLVcJi3MpeRXPuRyMzWDj4QTD4LZKBT7O1gS4KWER0MIW/4ozqbwcrKR30ZzprGs+DReUXkdRjhISOUnKs+HrZOXr9PNKzyM/TXnPwCclFIRojFztLHG1s6zUswAoKC7jfGou568Ii/MpeeyNSqeg5PJVvRZaNf6uSkj4V4SFv5stfq62uNlZyNiFOVCpwMpBebi1rXndshJlvENjYfSyjBYKs6ZO5edNm2jh7s7uo0evWn765ElmTZnCoYMH+e8rr/Do3LnGKkWIBmNtoaGTtyOdvB0rvV5erudiTiFRqXlEp+YTlZpLVGoep5Nz+PXERUrLL080Z2uhoZWLDX6utvi62uBb8dzaxQYvRys5jdYcaSquu2gARguFCWFhPDR7No888ECVy51dXHjjww/Z/NNPxipBiEZDrVbh5WiNl6M1/f91jVZpWTlxGQVEp+URnZpHTHo+sWn5nEnO4feTyRSXlRvW1ahVeDtZ0cpZCYlWlx7O1rR2sUGv1yPEjTBaKAwIDSUmOrra5S3c3Wnh7s7PmzcbqwQhmgStRm24apv2lZeVl+tJyi4kOi2PuPQCYtPziUnP50J6Pr+euEhqbuXTI6004HdoJ61dLoeG8myNj7ONDHyLa2oSYwrLFy9m+eLFACTGxREeHl6nz8nNza3ze5syc2x3c2yzO+BuCSFegBeAjqJSLakFepILyknO15OYVURGaT5HY3MJP6mnuLzyZzhaqnCzUtHCRoWbtRo3axVu1ipcrdS4Wquw0DS9sYzmuK9rw1jtbhKhEDZ9OmHTpwMwNCSEIZduN3idwsPD6/zepswc222ObYbK7dbr9aTkFhGblk9cRgEX0iueM5Tn/RcLKo1lALjZWdLS2RofZ2taOlnj7WhFS2dlLMPbyRpnG12jGwSXfV2/mkQoCCGun0qlwt3eCnd7K0L8rl5eWlbOxZwi4jMKiM/MJy69gPhM5XE8IZvtxy9SXFq5q2GlU+PlaK1MOuhkhZejFV6O1ng7KaHh5WiNg5W20QWHqD0JBSHMlFajpqWT0iMAl6uW6/V60vKKic8oIDGrgITMQhIyC0jMLiQpq5C/z6VxMaeIsn/1NuwstUpYVPQ0KoeG8ixjG42X0UJh2vjxRISHk5aaSpCPD88sXEhpiTLZ1NQZM7iYlMRNISHkZGejUqtZ9P77/H38OA4ODsYqSQhxHVQqFW52lrjZWdL1ihlpr1RaVk5KbhEJmYUVwXFFeGQVcjwh66rBcABnG50hLDwrgsPTQfnao+LZzlL+ZjUFo/2rL1m5ssblHp6eHI+LM9bmhRANQKtRG061Becq1yksKeNidiHxmQUkZhaSlH05NOIzC9kfk0FmfslV77Oz1OLpaIWnw6WgsMTDwQp3e0vcK15rITdUqncSxUIIo7LSaSouwLOtdp2CYiU4ErMKuZitBEdSVsUju5Bz51JJruJQFYCDBbQ8tAt3e0s8HCxpYW9ZMZZiiZu9ZUVvxwI7SxnrqA0JBSGEyVlbaC5fq1GNsnI96XnFXMwuJDmnkOTsIpKyC4k8GYXW3oqL2UWcTMomNbe4yvCw1KoNAXHpsJib/RVf21nSwt4CV1tLHK11Zjs3lYSCEKJJ0KhVtLBXegJweRqRcG0CQ4b0MnxfVq4nI7+Y5OwiUnMvP9Jyi0nJLSI1t5jErEKOxGeRlld1gGjUKlxsLXC1tagUJK6VvlaeXWwtmtXAuYSCEKJZ0agvD5BfS3lFgKTlFZOaU0RqXjFpFQGSlldESk4xqblFxKTnkZpTXGlSwyvZW2pxtbNQJkq0vfJZ+drt0mt2FjjbWKBpxL0QCQUhhNlSq1WGGW/bedhfc/28olLScotJzVOCQ+mBKL2P1Nwi0vOKiUnL50BMBhn5xVTRCUGlAidrXUVPxBJnW+XrS9+72inPLhWh4mxjgYW24SZBlFAQQohasrXUYmuppbWrzTXXLSvXk1VQYgiNtLxLPZBi0iu+zsgvJjo1nwMxmWTkV30oC5SeiLOtBQ/08+XBQQH13axKJBSEEMIILo1LuNha0Nbj2uuXl+vJLiwhNbeY9IrgSM0tJiNPCZKM/OKK8RTjklAQQohGQK1W4WRjgZON8W+kU2MdJt26EEKIRkVCQQghhIGEghBCCAMJBSGEEAYSCkIIIQwkFIQQQhhIKAghhDCQUBBCCGHQ5C5ei4qOpntISJ3em5aSgmuLFvVcUeNnju02xzaDebbbHNsM19/u2OjoWq2nytTrq55soxkaEhJC+P79pi6jwZlju82xzWCe7TbHNoPx2i2Hj4QQQhhIKAghhDDQPLNgwQJTF9GQuvXsaeoSTMIc222ObQbzbLc5thmM026zGlMQQghRMzl8JIQQwkBCQQghhIHZhMKv27YR0r493QMDee/1101djlHEXbjAyJtuonfHjvTt1IlFH3wAQEZ6OncNG0aPtm25a9gwMjMyTFypcZSVlTGoe3fGjhwJQHRUFEP79KFH27ZMGTuW4uJiE1dYvzIzM3lgzBh6dehA744d2bt7t1ns60/ee4++nTrRr3Nnpo0fT2FhYbPc17OmTiXQ3Z1+nTsbXqtu/+r1ep6eM4fugYH079KFyIMH67xdswiFsrIy5s6axZqtW9lz/DhrVq7k5PHjpi6r3mm1Wv73zjvsPXGC7X//zZeffMLJ48d57/XXGTx0KAfPnGHw0KHNNhQXffAB7Tt2NHy/YP58Zj7xBAfPnMHJ2ZlvliwxYXX175nHHuOWW29l38mTRBw6RLuOHZv9vk6Ij+fzDz9kx/797D56lLKyMn5ctapZ7usJYWGs2bat0mvV7d/tW7dy/swZDp45wweLF/PUI4/UebtmEQoH9u4lIDAQv4AALCwsuGfcOLasX2/qsuqdp5cX3Xr0AMDe3p52HTuSGB/PlvXrGT95MgDjJ09m808/mbJMo4iPi+OXzZu5/8EHAeUvp52//86dY8YAza/d2dnZ/LVzJ/dPmwaAhYUFTk5OZrGvy0pLKSwooLS0lIL8fDy9vJrlvh4QGoqzi0ul16rbv1vWr2fcAw+gUqno1bcvWZmZJCUm1mm7ZhEKifHxtGzVyvC9t48PifHxJqzI+GKioznyzz/07NOH5IsX8fTyApTgSElONnF19e/Zxx/npTffRK1WfqTT09JwdHJCq1Vmcmlu+zz6/HncWrRg5pQpDOrenUcffJC8vLxmv6+9W7Zk9ty5dG7dmvZeXjg4OtKtZ89mva+vVN3+rc/fcWYRClWedatSNXwhDSQ3N5cH7rmHV99/HwcHB1OXY3TbNm2ihbt7pXO2q9rnqma0z8tKSzl08CDTHnmEXf/8g42tbbM7VFSVzIwMtqxfz6GoKE4mJJCXl8f2rVuvWq857evaqM+fd7MIBW8fH+IvXDB8nxAXh5e3twkrMp6SkhIeuOce7p04kVF33w2Au4eHoSuZlJhIC3d3U5ZY7/b8+SdbN2wg2M+PaePGsfP333n28cfJysyktLQUUPa5ZzPa594+Pnj7+BDSpw8Ad44Zw+GDB5v9vg7/9Vd8/f1xa9ECnU7HHXffzd6//mrW+/pK1e3fqn7H1fXfwCxCoUevXpw7c4boqCiKi4v5cdUqbhs1ytRl1Tu9Xs/sadNo17Ejs5980vD6baNGsfKrrwBY+dVX3H7nnaYq0ShefO01jsfFcSQ6miWrVhF68818sWIFg266ifVr1gDNr90enp74tGrFmVOnAPjjt99oHxTU7Pe1T+vW7P/7b/Lz89Hr9YZ2N+d9faXq9qXX1gsAAAZ8SURBVO9to0ax6uuv0ev17Pv7bxwcHQ2Hma6X2VzR/MuWLTz7+OOUlZUxaepU5j73nKlLqne7IyK4bdAggoKDDcfWX3j1VUL69CHsvvuIi43Fp3Vrvvrhh6sGsJqLXeHhfPz226zetIno8+eZOm4cGenpdOnencXffoulpaWpS6w3hyMjmfPggxQXF+MXEMCny5ZRXl7e7Pf1qy++yLrVq9FqtQR3785HX35JYnx8s9vX08aPJyI8nLTUVNw9PHhm4UJG3nVXlftXr9czb/Zsft22DRsbGz5ZtqzOtxgwm1AQQghxbWZx+EgIIUTtSCgIIYQwkFAQQghhIKEghBDCQEJBCCGEgYSCaLScVCqee+opw/cfvf02r9XTjQIfCQsznNduTD/98AO9O3Zk5E03VXo9JjoaT2trBnbrZnis/PrretvurvBww2yxQlwPrakLEKI6lpaWbFy7lieffRZXNzdTl2NQVlaGRqOp1brfLFnC259+Sui/QgHAv00bIiIj67s8IW6I9BREo6XVagmbPp1P33vvqmX//ku/pZ0doPyFfPvgwYTddx8927VjwTPP8P2KFdzcuzf9g4OJOnfO8J7wX3/ltkGD6NmuHds2bQKUX/j/nTePm3r1on+XLiz7/HPD54686SYenDCB/sHBV9WzZuVK+gcH069zZ16cPx+AN156ib8jInhyxgz+O29erdvd0s6O5556itAePRg1dCipKSmAcrHaLX370r9LFyaOHm2YS//82bPcecstDOjaldAePQxtzM3NNdxv4aGJEw3z4yx45hn6BAXRv0sXnp87t9Z1CfMgoSAatQdnzeL7FSvIysqq9XuOHjrE6x98wF9HjrD6m284e/o0v+/dy/0PPsjnH31kWC82OprNf/zB95s38+SMGRQWFvLNkiU4ODqyY98+duzbx1dffEF0VBQAB/fu5flXXmHPv+7FkZiQwIL589nw++/siozk4L59bPrpJ+a/8ALdQkL4YsUKXn7rravqjDp3rtLho7927QIgLy+Prj16sPPgQQYMHswbCxfy/+3df0iUdxzA8bePj3h47Tlsk2XJZjkRggOxpWMaTyWzMJozmQ6VE+IkWS1k7c9o+ytrfwypwGLIsNv6wUHlqH/8I0jy1orWurCFrfMmyGikdd15KLrn2R/efXFL7QcbM/y84P54Hp77fj/cwX2e5/s9Ph+AFo+HLw4eJBAMstrt5kDifHNDA96dO+m7eZOeQIDXE+UNbt24QVt7Oz/evk04FOJKXx8PR0c5f/YsV/r7CQSDfLZ373N8G2IxkKQgFjTDMPjI4+HYoUPP/J6itWtZlp1Neno6uXl5bKyoAGC1281QOKyu+6C2Fk3TyMvP581Vqxi4c4eLPT2cOn6cssJCyktKGB0ZIXT37vS4xcXkrlz5xHw/XbtG6fr1vJaVha7rfNjQQKC396lxJpePkq93160DQNM0ttXVAVDX2MgPly8TiUR4/OgRZaYJQH1TE4HeXqLRKL8PD7O1uhoAh8NBRkaGindFTg6apuEuLGQoHOYVwyDd4eATr5fvz5xR1wqRJElBLHgft7bi6+xkbGxMndN1HcuygOlCgDPbL86seaNpmjrWNI0/E5U04cnSwikpKdi2zZeHD6sf6uDgoEoqTqdz1vj+60ox85VAnm/umZ9DamoqU1NT6LrOxatXeb+mhgvnzlGzefO/Gqt4+UlSEAte5tKlVNfW8u2MFotv5Oby8/XrAFzo7mZycvK5x+32+7Esi8F79/gtFCK/oIDyTZvo7OhQ4/06MPC3ZDSbt0tK6Lt0iZEHD6bbQ548SWnijv5FWJal9kv8J07wTlkZLpcLV2amWmI65fNRapoYhsHynBzOJzpwTUxMEI/H5xw7FovxOBKhorKSA+3t3JKNbvEP8u8j8VLYtWcPXx85oo6bmpupr6piY3ExZnn5nHfx83mroIAtpskf9+/z1dGjOBwOPF4vQ+EwZlERtm3zalYW3z2lteOy7Gw+b2tj64YN2LbNe5WVbHmG0s3JPYWkxu3badm9G6fTyS/9/Zhr1mC4XHxz+jQAHV1dfNrSQjweV1VRAY75fLTu2MH+fftIS0ujy++fc85YNEp9VRXj4+Ng2+yfZRNfLG5SJVWIBWbFkiUMx2L/dxhikZLlIyGEEIo8KQghhFDkSUEIIYQiSUEIIYQiSUEIIYQiSUEIIYQiSUEIIYTyF7dGFQpRgR2FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "dnn.plot_learning_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.8273\n"
     ]
    }
   ],
   "source": [
    "# Compute index values\n",
    "\n",
    "dnn.compute_index_values(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:210: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot misclassifications\n",
    "\n",
    "dnn.plot_misclassification(X_val, y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
