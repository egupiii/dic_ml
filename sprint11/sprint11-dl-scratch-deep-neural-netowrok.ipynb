{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to rewrite codes of 3 layers of neural network that I created on the previous sprint to expand them to any structures by using classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    Implement neural network classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_epoch : int\n",
    "        Number of epochs\n",
    "    \n",
    "    batch_size : int\n",
    "        Size of batch\n",
    "    \n",
    "    verbose : bool\n",
    "        True if outputting learning process\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : list\n",
    "        List of arrays of records of loss on train dataset\n",
    "    \n",
    "    val_loss : list\n",
    "        List of arrays of records of loss on validation dataset\n",
    "    \n",
    "    layers : list\n",
    "        List of layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_epoch, batch_size, verbose=True):\n",
    "        # Record hyperparameters as attribute\n",
    "        self.epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        # Prepare lists for arrays to record losses\n",
    "        self.layers = []\n",
    "    \n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers += [layer]\n",
    "    \n",
    "    \n",
    "    def forward_layer(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def backward_layer(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit neural network classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Correct values of train dataset\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation dataset\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples, )\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Fit\n",
    "        if self.verbose:\n",
    "            count = 0\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            # Initialize\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            \n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                get_mini_batch_val = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "                \n",
    "                for ((mini_X_train, mini_y_train), (mini_X_val_train, mini_y_val_train)) in zip(get_mini_batch, get_mini_batch_val):\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    Z3_val = self.forward_layer(mini_X_val_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                        L_val = loss.cross_entropy_loss(mini_y_val_train, Z3_val)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "                    dX_val = self.backward_layer(mini_y_val_train)\n",
    "            \n",
    "            \n",
    "            else:\n",
    "                for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                    # Forwardpropagation per iteration\n",
    "                    Z3 = self.forward_layer(mini_X_train)\n",
    "                    \n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        # Initialize\n",
    "                        loss = Loss()\n",
    "                        # Compute losses\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z3)\n",
    "                    \n",
    "                    # Backforwardpropagation per iteration\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "            \n",
    "            \n",
    "            # Output learning process if verbose is True\n",
    "            if self.verbose:\n",
    "                self.loss += [sum(L) / self.batch_size]\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    self.val_loss += [sum(L_val) / self.batch_size]\n",
    "                    print(\"{0}th loss: {1}, val_loss: {2}\".format(count+1, self.loss[count], self.val_loss[count]))\n",
    "                else:\n",
    "                    print(self.loss[count])\n",
    "                count += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict by neural network classifier.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Samples\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_samples, 1)\n",
    "            Results of prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        Z3 = self.forward_layer(X)\n",
    "        \n",
    "        return np.argmax(Z3, axis=1)\n",
    "    \n",
    "    \n",
    "    def plot_learning_record(self):\n",
    "        \"\"\"\n",
    "        Plot learning records.\n",
    "        \"\"\"\n",
    "        \n",
    "        plt.figure(facecolor=\"azure\", edgecolor=\"coral\")\n",
    "        \n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        \n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def compute_index_values(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Index values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"accuracy score:\", accuracy_score(y, y_pred))\n",
    "    \n",
    "    \n",
    "    def plot_misclassification(self, X_val, y_val, y_pred):\n",
    "        \"\"\"\n",
    "        Plot results of misclassification. Show \"Results of prediction/Corrects\" above images.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            Results of prediction\n",
    "        \n",
    "        y_val : ndarray, shape (n_samples,)\n",
    "            Correct labels of validation data\n",
    "        \n",
    "        X_val : ndarray, shape (n_samples, n_features)\n",
    "            Features of validation data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Number of results I want to plot\n",
    "        num = 36\n",
    "\n",
    "        true_false = y_pred==y_val\n",
    "        false_list = np.where(true_false==False)[0].astype(np.int)\n",
    "\n",
    "        if false_list.shape[0] < num:\n",
    "            num = false_list.shape[0]\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
    "        for i in range(num):\n",
    "            ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "            ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
    "            ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Class Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer from a layer of n_nodes1 to a layer of n_nodes2\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes of the previous layer\n",
    "    \n",
    "    n_nodes2 : int\n",
    "        Number of nodes of the following layer\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight\n",
    "    \n",
    "    B : ndarray, shape (n_nodes2,)\n",
    "        Bias\n",
    "    \n",
    "    Z : ndarray, shape (batch_size, n_nodes1)\n",
    "        Deepcopy of input\n",
    "    \n",
    "    dW : float\n",
    "        Gradient of weight\n",
    "    \n",
    "    dB : float\n",
    "        Gradient of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize self.W and self.B by using initializer method\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "        self.Z = 0\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.Z = copy.deepcopy(X)\n",
    "        \n",
    "        return np.dot(X, self.W) + self.B\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = np.average(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA) / dA.shape[0]\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Class Initialization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization by Gaussian distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Class Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] Class Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return 1 / (1+np.exp(-self.A))\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        return Z * (1-Z) * dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return np.tanh(self.A)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        return (1-Z**2) * dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax function\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray, shape (batch_size, ith n_nodes)\n",
    "        Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        A -= np.max(A)\n",
    "        \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        \n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_samples, 1)\n",
    "            Correct values\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.Z - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 5] Create a Class of ReLU\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "$x$ : a feature (scaler)\n",
    "\n",
    "<br />\n",
    "\n",
    "In addition, the following equation is a differentiation of $f(x)$ with respect to $x$ for backpropagation.\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"\n",
    "    ReLU function\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector from the previous layer of kth class\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        return np.where(self.A<=0, 0, self.A)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, ith n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        return np.where(self.A<=0, 0, 1) * dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 6] Initial Value of Weight\n",
    "\n",
    "<br />\n",
    "\n",
    "Xavier's initial value\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "$n$ : Number of nodes of the previous layer\n",
    "\n",
    "<br />\n",
    "\n",
    "He's initial value\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{2}{n}}\n",
    "$$\n",
    "\n",
    "$n$ : Number of nodes of the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by Xavier's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Initialize a weight by He's method, and initialize a bias.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a weight by Xavier's method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "            Number of nodes of the previous layer\n",
    "\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(2/n_nodes1)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Initialize a bias.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "            Number of nodes of the following layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n_nodes2,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 7] Optimization Method\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to implement AdaGrad.\n",
    "\n",
    "$$\n",
    "H_i^{\\prime}  = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\\n",
    "W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "$$\n",
    "\n",
    "$H_i$ : Sum of squares of all gradients up to the previous iterations about ith layer\n",
    "\n",
    "$H_i^{\\prime}$ : Updated $H_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    h : float\n",
    "        Sum of squares of all gradients up to the previous iterations about ith layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.h = 0\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.h += layer.dW * layer.dW\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW / np.sqrt(self.h+1e-7)\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"\n",
    "    Compute loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def cross_entropy_loss(self, y, y_pred):\n",
    "            \"\"\"\n",
    "            Cross entropy error\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            y : ndarray, shape (n_samples, 1)\n",
    "                Correct values\n",
    "\n",
    "            y_pred : ndarray, shape (n_samples, 1)\n",
    "                Predicted values\n",
    "\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            ndarray, shape (n_samples, 1)\n",
    "                Cross entropy error\n",
    "            \"\"\"\n",
    "            \n",
    "            return np.sum(-1*y*np.log(y_pred+1e-10), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "      Train dataset\n",
    "    \n",
    "    y : ndarray, shape (n_samples, 1)\n",
    "      Correct values\n",
    "    \n",
    "    batch_size : int\n",
    "      Size of batch\n",
    "    \n",
    "    seed : int\n",
    "      Seed of random numbers of Numpy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    \"\"\"\n",
    "    Dropout\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dropout_ratio : float\n",
    "        Ratio of dropout\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    mask : float\n",
    "        Mask\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        self.mask = None\n",
    "    \n",
    "    \n",
    "    def forward(self, X, train_flag=True):\n",
    "        if train_flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_ratio\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1-self.dropout_ratio)\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        return dA * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to validate classes that I created above by 3 layers of neural network with MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the MNIST dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform unit8 to float\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform correct labels that are 0 to 9 to \n",
    "\n",
    "# Initialize\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Fit\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "\n",
    "# Transform\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train dataset\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "dnn = ScratchDeepNeuralNetrowkClassifier(100, 10)\n",
    "\n",
    "# 1st layer\n",
    "dnn.add(FC(784, 400, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Dropout())\n",
    "dnn.add(Relu())\n",
    "\n",
    "# 2nd layer\n",
    "dnn.add(FC(400, 200, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Dropout())\n",
    "dnn.add(Relu())\n",
    "\n",
    "# 3rd layer\n",
    "dnn.add(FC(200, 10, SimpleInitializer(sigma=0.01), AdaGrad(lr=0.001)))\n",
    "dnn.add(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th loss: 1.7496707648913705, val_loss: 1.7626044906259082\n",
      "2th loss: 1.6301377351838842, val_loss: 1.5778009244102356\n",
      "3th loss: 1.5913800035165477, val_loss: 1.485317911266626\n",
      "4th loss: 1.5720856921581894, val_loss: 1.4319719274194487\n",
      "5th loss: 1.5581749409911483, val_loss: 1.3977993034577072\n",
      "6th loss: 1.5459806007269248, val_loss: 1.3728629213342778\n",
      "7th loss: 1.5341512006728835, val_loss: 1.3531908746018997\n",
      "8th loss: 1.5224463013503493, val_loss: 1.3366075860891988\n",
      "9th loss: 1.5104290935030424, val_loss: 1.3219368222534882\n",
      "10th loss: 1.498348840462283, val_loss: 1.3087976773531909\n",
      "11th loss: 1.4860842944490653, val_loss: 1.296873545872844\n",
      "12th loss: 1.474297540751824, val_loss: 1.2853197048062104\n",
      "13th loss: 1.4627398328360142, val_loss: 1.2748121472099425\n",
      "14th loss: 1.452070360910089, val_loss: 1.2649829744215242\n",
      "15th loss: 1.4416499398627758, val_loss: 1.2559616373321436\n",
      "16th loss: 1.4312223018283992, val_loss: 1.2474711381102737\n",
      "17th loss: 1.4210593016581468, val_loss: 1.2393027873816476\n",
      "18th loss: 1.411315636761627, val_loss: 1.231438460231451\n",
      "19th loss: 1.4017949367543596, val_loss: 1.2238888165728485\n",
      "20th loss: 1.392619437079326, val_loss: 1.216685629811187\n",
      "21th loss: 1.3836242809171027, val_loss: 1.209704302461601\n",
      "22th loss: 1.374996123550717, val_loss: 1.202980042934911\n",
      "23th loss: 1.3666600478113815, val_loss: 1.1965373137498378\n",
      "24th loss: 1.3586139443311498, val_loss: 1.1901499407198277\n",
      "25th loss: 1.3508461105350893, val_loss: 1.1839286208219175\n",
      "26th loss: 1.3432698364819846, val_loss: 1.177798653923025\n",
      "27th loss: 1.335981895668858, val_loss: 1.1719086591218422\n",
      "28th loss: 1.3289859951998584, val_loss: 1.166273490758085\n",
      "29th loss: 1.3222881060896234, val_loss: 1.1607181826091648\n",
      "30th loss: 1.3157644448158625, val_loss: 1.1554205457153448\n",
      "31th loss: 1.309414327400614, val_loss: 1.150381510015245\n",
      "32th loss: 1.3031929865267569, val_loss: 1.1455831336780649\n",
      "33th loss: 1.2971269150775269, val_loss: 1.140961300580904\n",
      "34th loss: 1.2912591341962345, val_loss: 1.1364097429726376\n",
      "35th loss: 1.2855339833116077, val_loss: 1.1320253595711427\n",
      "36th loss: 1.2799872755448365, val_loss: 1.1277157050847113\n",
      "37th loss: 1.2745842785913994, val_loss: 1.123559677412296\n",
      "38th loss: 1.2692320245026534, val_loss: 1.1194524582937544\n",
      "39th loss: 1.2640720899573383, val_loss: 1.115453364709865\n",
      "40th loss: 1.2591531230275705, val_loss: 1.11163583389076\n",
      "41th loss: 1.2544168283762964, val_loss: 1.1079836613687508\n",
      "42th loss: 1.2497797253589709, val_loss: 1.1043840102446656\n",
      "43th loss: 1.2452244967296486, val_loss: 1.1009032464343969\n",
      "44th loss: 1.2407247315923828, val_loss: 1.0975556405917095\n",
      "45th loss: 1.2363796477412294, val_loss: 1.0942381849135168\n",
      "46th loss: 1.2321825383013119, val_loss: 1.0910488405561243\n",
      "47th loss: 1.2280516157672865, val_loss: 1.0879202520491813\n",
      "48th loss: 1.224094026686597, val_loss: 1.084884103298656\n",
      "49th loss: 1.220220006904364, val_loss: 1.0819015280272963\n",
      "50th loss: 1.2164934552238413, val_loss: 1.0790224009522462\n",
      "51th loss: 1.2128279596582392, val_loss: 1.0762048062878955\n",
      "52th loss: 1.2093513761132524, val_loss: 1.0734916714905833\n",
      "53th loss: 1.205903866410278, val_loss: 1.0708431870263437\n",
      "54th loss: 1.202548922986904, val_loss: 1.0682760428029607\n",
      "55th loss: 1.1993744414630703, val_loss: 1.0657599829015323\n",
      "56th loss: 1.1963604651725532, val_loss: 1.063204849774879\n",
      "57th loss: 1.1933718372996684, val_loss: 1.0607313460243528\n",
      "58th loss: 1.190425125076867, val_loss: 1.0583420104916152\n",
      "59th loss: 1.1875704054921414, val_loss: 1.056004671102524\n",
      "60th loss: 1.184799682798376, val_loss: 1.0537556128039145\n",
      "61th loss: 1.182153198315221, val_loss: 1.0515401747774011\n",
      "62th loss: 1.1795568575828592, val_loss: 1.0493397361118562\n",
      "63th loss: 1.1769952369965897, val_loss: 1.047249785662903\n",
      "64th loss: 1.1744808426320599, val_loss: 1.045184149854712\n",
      "65th loss: 1.1720496066378572, val_loss: 1.0431701765929802\n",
      "66th loss: 1.1697379477384615, val_loss: 1.0411705852342859\n",
      "67th loss: 1.1674932204171262, val_loss: 1.0391206414835596\n",
      "68th loss: 1.1652715297146137, val_loss: 1.037160318695357\n",
      "69th loss: 1.163057830891184, val_loss: 1.035278335450159\n",
      "70th loss: 1.1608972264151944, val_loss: 1.0334402761563144\n",
      "71th loss: 1.158754879530685, val_loss: 1.0316451230845494\n",
      "72th loss: 1.156691272771625, val_loss: 1.029863837785213\n",
      "73th loss: 1.15472685005423, val_loss: 1.028094332286197\n",
      "74th loss: 1.1528195960116538, val_loss: 1.0263927468441483\n",
      "75th loss: 1.1509609726717156, val_loss: 1.0246724341562632\n",
      "76th loss: 1.1490877865262052, val_loss: 1.0230184689206445\n",
      "77th loss: 1.1472099510939289, val_loss: 1.0213240551753588\n",
      "78th loss: 1.1453358648683445, val_loss: 1.0196855571257724\n",
      "79th loss: 1.1435923388018412, val_loss: 1.0180976351078552\n",
      "80th loss: 1.1418609780090225, val_loss: 1.0165773458921399\n",
      "81th loss: 1.140138333307499, val_loss: 1.0151428374726315\n",
      "82th loss: 1.1384592494956518, val_loss: 1.0137054505634655\n",
      "83th loss: 1.136846893833347, val_loss: 1.0123324537163456\n",
      "84th loss: 1.1351771752155648, val_loss: 1.0109460808666322\n",
      "85th loss: 1.133570308866433, val_loss: 1.009605336964776\n",
      "86th loss: 1.132039521283446, val_loss: 1.0082459435946767\n",
      "87th loss: 1.1305290580294554, val_loss: 1.0069007928876028\n",
      "88th loss: 1.1290242118721192, val_loss: 1.0056817484038874\n",
      "89th loss: 1.1275738050175836, val_loss: 1.0044224738116347\n",
      "90th loss: 1.126244857298963, val_loss: 1.003230604903509\n",
      "91th loss: 1.1249350910732376, val_loss: 1.002063128392932\n",
      "92th loss: 1.1236520641180512, val_loss: 1.000972744439237\n",
      "93th loss: 1.1224025688212955, val_loss: 0.9998187483794796\n",
      "94th loss: 1.1211280962397145, val_loss: 0.9987029521881216\n",
      "95th loss: 1.1199079462987076, val_loss: 0.997503242239822\n",
      "96th loss: 1.1186631653744035, val_loss: 0.9963532776061328\n",
      "97th loss: 1.11746556326225, val_loss: 0.9951909630337609\n",
      "98th loss: 1.1162611045293702, val_loss: 0.9940367307677354\n",
      "99th loss: 1.1150906223091395, val_loss: 0.9929855645770438\n",
      "100th loss: 1.114029844989206, val_loss: 0.9919006389580094\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "\n",
    "dnn.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "\n",
    "y_pred = dnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curves\n",
    "\n",
    "dnn.plot_learning_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.8249\n"
     ]
    }
   ],
   "source": [
    "# Compute index values\n",
    "\n",
    "dnn.compute_index_values(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kazukiegusa/.pyenv/versions/anaconda3-5.3.0/lib/python3.6/site-packages/ipykernel_launcher.py:210: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot misclassifications\n",
    "\n",
    "dnn.plot_misclassification(X_val, y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
