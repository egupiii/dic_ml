{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class of logistic regression from scratch\n",
    "\n",
    "class ScratchLogisticRegression():\n",
    "    \"\"\"\n",
    "    Implement logistic regression from scratch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter: int\n",
    "        The number of iteration\n",
    "\n",
    "    lr: float\n",
    "        Learning rate\n",
    "\n",
    "    reg: float\n",
    "        Regularization parameter\n",
    "\n",
    "    bias: bool\n",
    "        True if input the bias term\n",
    "\n",
    "    verbose: bool\n",
    "        True if output the learning process\n",
    "\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_: ndarray, shape(n_features,)\n",
    "        parameters\n",
    "\n",
    "    self.loss: ndarray, shape(self.iter,)\n",
    "        records of loss on train dataset\n",
    "\n",
    "    self.val_loss: ndarray, shape(self.iter,)\n",
    "        records of loss on validation dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_iter, lr, reg, bias, verbose):\n",
    "        # Record hyperparameters as attribute\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Prepare arrays for recording loss\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit datasets by logistic regression. In a case of inputting validation dataset, return the loss\n",
    "        and the accuracy of datasets per iteration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "\n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "\n",
    "        X_val: ndarray, shape(n_samples,n_features)\n",
    "            Features of validation dataset\n",
    "\n",
    "        y_val: ndarray, shape(n_samples,)\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "\n",
    "        ###print(\"fit-1, X=\",X.shape)   # (80,2)\n",
    "        ###print(\"fit-2, y=\",y.shape)   # (80,)\n",
    "        ###print(\"fit-101, X_val=\",X_val.shape)   # (20,2)\n",
    "        ###print(\"fit-102, y_val=\",y_val.shape)   # (20,)\n",
    "\n",
    "        # Change objective vectors to matrixes\n",
    "        y = y.reshape(len(y), 1)\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            y_val = y_val.reshape(len(y_val), 1)\n",
    "\n",
    "        # Add a bias term if self.bias is True\n",
    "        if self.bias == True:\n",
    "            # Create arrays of biases\n",
    "            X_bias = np.array([1 for _ in range(X.shape[0])])\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                X_val_bias = np.array([1 for _ in range(X_val.shape[0])])\n",
    "\n",
    "            # Change the vectors to matrixes\n",
    "            X_bias = X_bias.reshape(len(X_bias), 1)\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                X_val_bias = X_val_bias.reshape(len(X_val_bias), 1)\n",
    "            ###print(\"fit-3, X_bias=\",X_bias.shape)   # (80,1)\n",
    "            ###print(\"fit-4, X_val_bias=\",X_val_bias.shape)   # (20,1)\n",
    "\n",
    "            # Add the biases\n",
    "            X = np.hstack((X_bias, X))\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                X_val = np.hstack((X_val_bias, X_val))\n",
    "\n",
    "        # Change the arrays to lists\n",
    "        y = y.tolist()\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            y_val = y_val.tolist()\n",
    "\n",
    "        # Change the original values of the lists to only 2 kinds of values, 0 and 1\n",
    "        y = [1 if i == max(y) else 0 for i in y]\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            y_val = [1 if i == max(y_val) else 0 for i in y_val]\n",
    "\n",
    "        # Change the lists to arrays\n",
    "        y = np.array(y)\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            y_val = np.array(y_val)\n",
    "\n",
    "        # Change the vectors to matrixes\n",
    "        y = y.reshape(1, len(y))\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            y_val = y_val.reshape(1, len(y_val))\n",
    "\n",
    "        # Transform dataframes to move their features to rows\n",
    "        X = X.T\n",
    "        if (X_val is not None) and (y_val is not None):\n",
    "            X_val = X_val.T\n",
    "\n",
    "        ###print(\"fit-5, X=\",X.shape)   # (3,80)\n",
    "        ###print(\"fit-6, y=\",y.shape)   # (1,80)\n",
    "        ###if (X_val is not None) and (y_val is not None):\n",
    "        ###print(\"fit-103, X_val=\",X_val.shape)   # (3,20)\n",
    "        ###print(\"fit-104, y_val=\",y_val.shape)   # (1,20)\n",
    "\n",
    "        # Set a hypothesis parameter randomly and transform it\n",
    "        self.coef_ = np.random.randn(X.shape[0])\n",
    "\n",
    "        # Change the vector to a matrix\n",
    "        self.coef_ = self.coef_.reshape(len(self.coef_), 1)\n",
    "        ###print(\"fit-7, self.coef_=\",self.coef_.shape)   # (3,1)\n",
    "\n",
    "        # Update the parameter and get loss of train dataset\n",
    "        for i in range(self.iter):\n",
    "            # Update the parameter\n",
    "            self.coef_ = self._gradient_descent(X, y)\n",
    "            ###print(\"fit-8, self.coef_=\",self.coef_.shape)   # (3,1)\n",
    "\n",
    "            # Compute the cross entropy\n",
    "            cross_entropy = self._compute_cost(X, y)\n",
    "            ###print(\"fit-9, cross_entropy=\",cross_entropy.shape)   # ()\n",
    "\n",
    "            # Record the errors\n",
    "            self.loss[i] = cross_entropy\n",
    "\n",
    "            # Return the loss if verbose is True\n",
    "            if self.verbose:\n",
    "                print(self.loss[i])\n",
    "\n",
    "            # Get loss of validation datasets\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                # Get the cross entropy\n",
    "                val_cross_entropy = self._compute_cost(X_val, y_val)\n",
    "\n",
    "                # Record the errors\n",
    "                self.val_loss[i] = val_cross_entropy\n",
    "\n",
    "                # Return the loss if verbose is True\n",
    "                if self.verbose:\n",
    "                    print(self.val_loss[i])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict by logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Samples\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape(n_samples,1)\n",
    "            Results of the prediction\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.predict_proba(X)\n",
    "\n",
    "        # Change a probability that is more than 0.5 to 1 and a probability that is less than or equals to 0.5 to 0\n",
    "        pred_list = []\n",
    "        for i in y_pred[0]:\n",
    "            pred_list.append(round(i))\n",
    "\n",
    "        # Change the values of the list to integers\n",
    "        pred_list = list(map(int, pred_list))\n",
    "\n",
    "        return pred_list\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Probability estimation for logistic regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Samples\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            Probability of\n",
    "        \"\"\"\n",
    "\n",
    "        ###print(\"predict_proba-1, X=\",X.shape)   # (20,2)\n",
    "\n",
    "        # Add a bias if self.bias is True\n",
    "        if self.bias == True:\n",
    "            # Create arrays of biases\n",
    "            X_bias = np.array([1 for _ in range(X.shape[0])])\n",
    "\n",
    "            # Change the vectors to a matrix\n",
    "            X_bias = X_bias.reshape(len(X_bias), 1)\n",
    "\n",
    "            # Add the biases\n",
    "            X = np.hstack((X_bias, X))\n",
    "\n",
    "        ###print(\"predict_proba-2, self.coef_=\",self.coef_.shape)   # (3,1)\n",
    "        ###print(\"predict_proba-3, X=\",X.shape)   # (20,3)\n",
    "\n",
    "        # Predict train dataset\n",
    "        y_pred = self._linear_hypothesis(X.T)  # (1,3) * (3,20)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    # Create a definition of sigmoid function\n",
    "    def _sigmoid_function(self, z):\n",
    "        \"\"\"\n",
    "        Return sigmoid function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z: int\n",
    "            Index of natural logarithm of sigmoid function\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        float\n",
    "            Results of computation by sigmoid function\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute sigmoid function\n",
    "        return 1 / (1 + math.e ** (-z))\n",
    "\n",
    "\n",
    "    # Create a definition of hypothesis function\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        Return hypothesis function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Train dataset\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape(n_samples,1)\n",
    "            Results of the prediction by hypothesis function\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute an index of linear hypothesis\n",
    "        z = np.dot(self.coef_.T, X)  # (1,3) * (3,80)\n",
    "\n",
    "        # Compute the hypothesis function\n",
    "        y_pred = self._sigmoid_function(z)\n",
    "        ###print(\"_linear_hypothesis-1, y_pred=\",y_pred.shape)   # (1,80) <-> (1,20)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    # Create a definition to compute the cross entropy\n",
    "    def _compute_cost(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the cross entropy. Import the \"cross_entropy\" definition.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            train dataset\n",
    "\n",
    "        y: ndarray, shape(n_samples,1)\n",
    "            correct value\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape(1,)\n",
    "            cross entropy\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self._linear_hypothesis(X)\n",
    "\n",
    "        return self.cross_entropy(y_pred, y)\n",
    "\n",
    "\n",
    "    # Create a definition of the cross entropy\n",
    "    def cross_entropy(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        Compute the cross entropy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred: ndarray, shape(n_samples,)\n",
    "            predited value\n",
    "\n",
    "        y: ndarray, shape(n_samples,)\n",
    "            correct value\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        cross_entropy: numpy.float\n",
    "            cross entropy\n",
    "        \"\"\"\n",
    "\n",
    "        ###print(\"cross_entropy-1, y_pred=\",y_pred.shape)   # (1,80) <-> (1,20)\n",
    "\n",
    "        # Compute a probability that equals to 1\n",
    "        prob1 = -y * np.log(y_pred)\n",
    "\n",
    "        # Compute a probability that equals to 0\n",
    "        prob0 = (1 - y) * np.log(1 - y_pred)\n",
    "\n",
    "        # Compute the joint probability\n",
    "        joint_prob = prob1 - prob0\n",
    "\n",
    "        # Sum the joint probabilities\n",
    "        sum_joint_probs = np.sum(joint_prob)\n",
    "\n",
    "        # Compute a regularization term\n",
    "        term_reg = self.reg / (2 * y.shape[1]) * np.sum(self.coef_ ** 2)\n",
    "\n",
    "        # Compute the cross entropy\n",
    "        return sum_joint_probs / y.shape[1] + term_reg\n",
    "\n",
    "\n",
    "    # Create a definition to fit datasets by steepest descent method\n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit datasets by steepest descent method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            train dataset\n",
    "\n",
    "        y: ndarray, shape(n_samples,1)\n",
    "            correct value\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape(1,)\n",
    "            parameter(weight)\n",
    "        \"\"\"\n",
    "\n",
    "        # Predict train dataset\n",
    "        y_pred = self._linear_hypothesis(X)\n",
    "        ###print(\"_gradient_decsent-1, y_pred=\",y_pred.shape)  # (1,80)\n",
    "\n",
    "        ###print(\"_gradient_decsent-2, y=\",y.shape)   # (1,80)\n",
    "\n",
    "        # Compute the error\n",
    "        error = y_pred - y\n",
    "        ###print(\"_gradient_decsent-3, error=\",error.shape)   # (1,80)\n",
    "\n",
    "        # Compute the gradient\n",
    "        grad = np.dot(X, error.T)  # (3,80) * (80,1)\n",
    "        ###print(\"_gradient_decsent-4, grad=\",grad.shape)   # (3,1)\n",
    "\n",
    "        # Sum gradients\n",
    "        sum_grads = np.sum(grad)\n",
    "\n",
    "        # Compute the regularization term\n",
    "        reg_term = self.reg / y.shape[1] * self.coef_\n",
    "        ###print(\"_gradient_descent-5, reg_term=\",reg_term)\n",
    "\n",
    "        # Update the parameter\n",
    "        if self.bias == False:\n",
    "            return self.coef_ - self.lr * (grad / y.shape[1] + reg_term)\n",
    "        else:\n",
    "            # Change the (1,1) element to 0\n",
    "            reg_term[0, 0] = 0\n",
    "            ###print(\"_gradient_descent-6, reg_term=\",reg_term)\n",
    "            return self.coef_ - self.lr * (grad / y.shape[1] + reg_term)\n",
    "\n",
    "\n",
    "    # Plot learning records\n",
    "    def plot_learning_record(self):\n",
    "        \"\"\"\n",
    "        Plot learning records.\n",
    "        \"\"\"\n",
    "\n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "\n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Iterrations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Compute index values\n",
    "    def compute_index_values(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Index values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_samples,n_features)\n",
    "            Features of train dataset\n",
    "\n",
    "        y: ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        # Change values of y to only 2 kinds of values, 0 and 1\n",
    "        y = [1 if i == max(y) else 0 for i in y]\n",
    "\n",
    "        ###print(\"compute_index_values-1, y_pred=\", y_pred)\n",
    "        ###print(\"compute_index_values-2, y=\", y)\n",
    "\n",
    "        ###print(\"compute_index_values-3, y_pred=\", len(y_pred))   # 20\n",
    "        ###print(\"compute_index_values-4, y=\", len(y))   # 20\n",
    "\n",
    "        # Return index values\n",
    "        print(\"accuracy score: \", accuracy_score(y, y_pred))\n",
    "        print(\"precision score: \", precision_score(y, y_pred))\n",
    "        print(\"recall score: \", recall_score(y, y_pred))\n",
    "        print(\"f1 score: \", precision_score(y, y_pred))\n",
    "        print(\"confusion matrix:\")\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "\n",
    "\n",
    "    def decision_boundary(self, X, y, step=0.01, title=\"Decision Boundary\", xlabel=\"Number of Iteration\", ylabel=\"Loss\",\n",
    "                          target_names=[\"setosa\", \"virginica\"]):\n",
    "        \"\"\"\n",
    "        Plot a decision boundary of a model fitting binary classification by 2-dimentional features.\n",
    "\n",
    "        Parameters\n",
    "        ----------------\n",
    "        X : ndarray, shape(n_samples, 2)\n",
    "            Features of train dataset\n",
    "\n",
    "        y : ndarray, shape(n_samples,)\n",
    "            Correct values of train dataset\n",
    "\n",
    "        step : float, (default : 0.1)\n",
    "            Set intervals to compute the prediction\n",
    "\n",
    "        title : str\n",
    "            Input the title of the graph\n",
    "\n",
    "        xlabel, ylabel : str\n",
    "            Input names of each axis\n",
    "\n",
    "        target_names= : list of str\n",
    "            Input a list of the legends\n",
    "        \"\"\"\n",
    "\n",
    "        # Setting\n",
    "        scatter_color = [\"r\", \"b\"]\n",
    "        contourf_color = [\"pink\", \"skyblue\"]\n",
    "        n_class = 2\n",
    "\n",
    "        # Predict\n",
    "        mesh_f0, mesh_f1 = np.meshgrid(np.arange(np.min(X[:, 0]) - 0.5, np.max(X[:, 0]) + 0.5, step),\n",
    "                                       np.arange(np.min(X[:, 1]) - 0.5, np.max(X[:, 1]) + 0.5, step))\n",
    "        mesh = np.c_[np.ravel(mesh_f0), np.ravel(mesh_f1)]\n",
    "        pred = self.predict_proba(mesh).reshape(mesh_f0.shape)\n",
    "\n",
    "        # Plot\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.contourf(mesh_f0, mesh_f1, pred, n_class - 1, cmap=ListedColormap(contourf_color))\n",
    "        plt.contour(mesh_f0, mesh_f1, pred, n_class - 1, colors='y', linewidths=3, alpha=0.5)\n",
    "        for i, target in enumerate(set(y)):\n",
    "            plt.scatter(X[y == target][:, 0], X[y == target][:, 1], s=80, color=scatter_color[i], label=target_names[i],\n",
    "                        marker='o')\n",
    "        patches = [mpatches.Patch(color=scatter_color[i], label=target_names[i]) for i in range(n_class)]\n",
    "        plt.legend(handles=patches)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
