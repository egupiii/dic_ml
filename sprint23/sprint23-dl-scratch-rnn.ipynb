{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to implement RNN from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization by Gaussian distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, m, n):\n",
    "        \"\"\"\n",
    "        Initialization of weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int\n",
    "            Number of features/nodes\n",
    "\n",
    "        n : int\n",
    "            Number of nodes\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (m, n)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(m, n)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n):\n",
    "        \"\"\"\n",
    "        Initialization of biases\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : real number\n",
    "            Random real number\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(n, 1)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (n_sequences, n_nodes)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            State/output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = np.tanh(A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "#     def backward(self, dA):\n",
    "#         \"\"\"\n",
    "#         Back propagation\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         dA : ndarray, shape (batch_size, n_nodes2)\n",
    "#             Gradient given from the following layer\n",
    "        \n",
    "#         Returns\n",
    "#         -------\n",
    "#         ndarray, shape (batch_size, ith n_nodes)\n",
    "#             Output\n",
    "#         \"\"\"\n",
    "        \n",
    "#         Z = self.forward(self.A)\n",
    "        \n",
    "#         d_tanh = (1 - Z**2)*dA\n",
    "        \n",
    "#         return d_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Implement Forward Propagation of SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + b\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$a_t$ : State before passing an activation function at time $t$\n",
    "\n",
    "$h_t$ : State/output at time $t$\n",
    "\n",
    "$x_t$ : Input at time $t$\n",
    "\n",
    "$W_x$ : Weight for input\n",
    "\n",
    "$h_{t-1}$ : State at time $t-1$ (Forward propagation from a previous time)\n",
    "\n",
    "$W_h$ : Weight for state\n",
    "\n",
    "$b$ : Bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Implement Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Equations\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha E(\\frac{\\partial L}{\\partial W_x}) \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha E(\\frac{\\partial L}{\\partial W_h}) \\\\\n",
    "b^{\\prime} = b - \\alpha E(\\frac{\\partial L}{\\partial b})\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$\\alpha$ : Learning rate\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : Gradient of a loss $L$ about $W_x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : Gradient of a loss $L$ about $W_h$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : Gradient of a loss $L$ about $b$\n",
    "\n",
    "$E()$ : Computation of a mean of a vector in a direction of a mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation to Compute Gradients\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} Ã—(1-tanh^2(a_t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_t}$ : Sum of an error of a state and an error of an output at a previous time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations of Errors Given to a Previous Time/Layer\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        Size of a batch\n",
    "    \n",
    "    n_features : int\n",
    "        Number of features\n",
    "    \n",
    "    n_nodes : int\n",
    "        Number of nodes\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W_input : ndarray, shape (n_features, n_nodes)\n",
    "        Weight for input\n",
    "    \n",
    "    W_state : ndarray, shape (n_nodes, n_nodes)\n",
    "        Weight for input\n",
    "    \n",
    "    B : ndarray, shape (1,)\n",
    "        Bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size, n_sequences, n_features, n_nodes, initializer):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_sequences = n_sequences\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.initializer = initializer\n",
    "        \n",
    "        # Initialize weights and biases by using initializer methods\n",
    "        self.W_input = self.initializer.W(self.n_features, self.n_nodes)\n",
    "        self.W_state = self.initializer.W(self.n_nodes, self.n_nodes)\n",
    "        self.B = self.initializer.B(1)\n",
    "        \n",
    "        self.a = []\n",
    "        self.h = np.zeros((batch_size, n_nodes))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_sequences, n_features)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            State/Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        a = np.dot(X, self.W_input) + np.dot(self.h, self.W_state) + self.B\n",
    "        self.a.append(a)\n",
    "        \n",
    "        tanh = Tanh()\n",
    "        self.h = tanh.forward(a)\n",
    "        \n",
    "        return self.h\n",
    "    \n",
    "    \n",
    "    def backward(self, X, a, state, previous_state output):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Activation function layer\n",
    "        tanh = Tanh()\n",
    "        p = (state + output) * (1 - tanh(a)**2)\n",
    "        \n",
    "        # Update\n",
    "        self.b = p\n",
    "        self.W_input = np.dot(X.T, p)\n",
    "        self.W_state = np.dot(previous_state.T, p)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Validate Forward Propagation by Using Small Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Input\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])\n",
    "\n",
    "# Output\n",
    "correct_h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of forward propagation\n",
    "\n",
    "def forward(x, w_x, w_h, h, b):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_sequences, n_features)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            State/Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        a = np.dot(x, w_x) + np.dot(h, w_h) + b\n",
    "        \n",
    "        tanh = Tanh()\n",
    "        h = tanh.forward(a)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "[[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# Forward propagtion\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    h = forward(x[0][i], w_x, w_h, h, b)\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "print(h)\n",
    "print(correct_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
