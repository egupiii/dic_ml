{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation from Scratch\n",
    "\n",
    "<br />\n",
    "\n",
    "I am going to implement RNN from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Fully connected layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "        Number of nodes of the previous layer\n",
    "    \n",
    "    n_nodes2 : int\n",
    "        Number of nodes of the following layer\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W : ndarray, shape (n_nodes1, n_nodes2)\n",
    "        Weight\n",
    "    \n",
    "    B : ndarray, shape (n_nodes2,)\n",
    "        Bias\n",
    "    \n",
    "    Z : ndarray, shape (batch_size, n_nodes1)\n",
    "        Deepcopy of input\n",
    "    \n",
    "    dW : float\n",
    "        Gradient of weight\n",
    "    \n",
    "    dB : float\n",
    "        Gradient of bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize self.W and self.B by using initializer method\n",
    "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "        self.Z = 0\n",
    "        self.dW = 0\n",
    "        self.dB = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (batch_size, n_nodes1)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes2)\n",
    "            Output\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.Z = copy.deepcopy(X)\n",
    "        \n",
    "        return np.dot(X, self.W) + self.B\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backwardpropagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient given from the following layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient given to the next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dB = np.average(dA)\n",
    "        self.dW = np.dot(self.Z.T, dA) / dA.shape[0]\n",
    "        \n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization by Gaussian distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "        Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    \n",
    "    def W(self, m, n):\n",
    "        \"\"\"\n",
    "        Initialization of weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        m : int\n",
    "            Number of features/nodes\n",
    "\n",
    "        n : int\n",
    "            Number of nodes\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W : ndarray, shape (m, n)\n",
    "            Weight\n",
    "        \"\"\"\n",
    "\n",
    "        W = self.sigma * np.random.randn(m, n)\n",
    "        \n",
    "        return W.astype(\"f\")\n",
    "    \n",
    "    \n",
    "    def B(self, n):\n",
    "        \"\"\"\n",
    "        Initialization of biases\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : real number\n",
    "            Random real number\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        B : ndarray, shape (n,)\n",
    "            Bias\n",
    "        \"\"\"\n",
    "\n",
    "        B = self.sigma * np.random.randn(n, 1)\n",
    "        \n",
    "        return B.astype(\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_FC:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent for Fully Connected layer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_RNN:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent for RNN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases of layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of preupdated layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        layer : Instance\n",
    "            Instance of updated layer\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W_input -= self.lr * layer.dW\n",
    "        layer.W_state -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (n_sequences, n_nodes)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            State/Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.A = A\n",
    "        \n",
    "        Z = np.tanh(self.A)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Back propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes)\n",
    "            Sum of an error of state and an error of output from a previous time\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward(self.A)\n",
    "        \n",
    "        d_tanh = dA * (1 - Z**2)\n",
    "        \n",
    "        return d_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    Z : ndarray, shape (batch_size, n_nodes)\n",
    "        Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        A : ndarray, shape (batch_size,)\n",
    "            Vector\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            Output\n",
    "        \"\"\"\n",
    "        \n",
    "        A -= np.max(A)\n",
    "        \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        \n",
    "        self.Z = Z\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_sequences, 1)\n",
    "            Correct values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (batch_size,)\n",
    "            Probability vector of kth class\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.Z - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch():\n",
    "    \"\"\"\n",
    "    Iterator to get a mini-batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_sequences, n_features)\n",
    "      Train dataset\n",
    "    \n",
    "    y : ndarray, shape (n_sequences, 1)\n",
    "      Correct values\n",
    "    \n",
    "    batch_size : int\n",
    "      Size of batch\n",
    "    \n",
    "    seed : int\n",
    "      Seed of random numbers of Numpy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        \n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def cross_entropy_loss(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Cross entropy error\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : ndarray, shape (n_sequences, 1)\n",
    "            Correct values\n",
    "\n",
    "        y_pred : ndarray, shape (n_sequences, 1)\n",
    "            Predicted values\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_sequences, 1)\n",
    "            Cross entropy error\n",
    "        \"\"\"\n",
    "\n",
    "        return np.sum(-1*y*np.log(y_pred+1e-10), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Implement Forward Propagation of SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + b\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$a_t$ : State before passing an activation function at time $t$\n",
    "\n",
    "$h_t$ : State/output at time $t$\n",
    "\n",
    "$x_t$ : Input at time $t$\n",
    "\n",
    "$W_x$ : Weight for input\n",
    "\n",
    "$h_{t-1}$ : State at time $t-1$ (Forward propagation from a previous time)\n",
    "\n",
    "$W_h$ : Weight for state\n",
    "\n",
    "$b$ : Bias term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Implement Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Equations\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha E(\\frac{\\partial L}{\\partial W_x})\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_h^{\\prime} = W_h - \\alpha E(\\frac{\\partial L}{\\partial W_h})\n",
    "$$\n",
    "\n",
    "$$\n",
    "b^{\\prime} = b - \\alpha E(\\frac{\\partial L}{\\partial b})\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$\\alpha$ : Learning rate\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : Gradient of a loss $L$ about $W_x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : Gradient of a loss $L$ about $W_h$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : Gradient of a loss $L$ about $b$\n",
    "\n",
    "$E()$ : Computation of a mean of a vector in a direction of a mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation to Compute Gradients\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "<br />\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_t}$ : Sum of an error of a state and an error of an output at a previous time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations of Errors Given to a Previous Time/Layer\n",
    "\n",
    "<br />\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        Size of a batch\n",
    "    \n",
    "    n_features : int\n",
    "        Number of features\n",
    "    \n",
    "    n_nodes : int\n",
    "        Number of nodes\n",
    "    \n",
    "    initializer : Instance\n",
    "        Instance of initialization method\n",
    "    \n",
    "    optimizer : Instance\n",
    "        Instance of optimisation method\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    W_input : ndarray, shape (n_features, n_nodes)\n",
    "        Weight for input\n",
    "    \n",
    "    W_state : ndarray, shape (n_nodes, n_nodes)\n",
    "        Weight for input\n",
    "    \n",
    "    B : ndarray, shape (1,)\n",
    "        Bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size, n_sequences, n_features, n_nodes, initializer, optimizer):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_sequences = n_sequences\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize weights and biases by using initializer methods\n",
    "        self.W_input = self.initializer.W(self.n_features, self.n_nodes)\n",
    "        self.W_state = self.initializer.W(self.n_nodes, self.n_nodes)\n",
    "        self.B_RNN = self.initializer.B(1)\n",
    "        \n",
    "        self.X_save = 0\n",
    "        self.a = []\n",
    "        self.h = np.zeros((batch_size, n_nodes))\n",
    "        self.dW_input = 0\n",
    "        self.dW_state = 0\n",
    "        self.dB_RNN = 0\n",
    "        self.d_state = 0\n",
    "        self.d_input = 0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_sequences, n_features)\n",
    "            Input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        ndarray, shape (batch_size, n_nodes)\n",
    "            State/Output\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_save = X\n",
    "        \n",
    "        A = np.dot(X, self.W_input) + np.dot(self.h, self.W_state) + self.B_RNN\n",
    "        self.a.append(A)\n",
    "        \n",
    "        tanh = Tanh()\n",
    "        self.h = tanh.forward(a)\n",
    "        \n",
    "        return self.h\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : ndarray, shape (batch_size, n_nodes)\n",
    "            Gradient given from a next layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # Activation function layer\n",
    "        tanh = Tanh()\n",
    "        d_tanh = tanh.backward(dA)\n",
    "        \n",
    "        self.dB_RNN = d_tanh\n",
    "        self.dW_input = np.dot(self.X_save.T, d_tanh)\n",
    "        self.dW_state = np.dot(d_tanh.T, d_tanh)\n",
    "        \n",
    "        self.d_state = np.dot(d_tanh, self.W_state.T)\n",
    "        self.d_input = np.dot(d_tanh, self.W_input.T)\n",
    "        \n",
    "        # Update\n",
    "        self = self.optimizer.update(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Validate Forward Propagation by Using Small Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Input\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])\n",
    "\n",
    "# Output\n",
    "correct_h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of forward propagation\n",
    "\n",
    "def forward(x, w_x, w_h, h, b):\n",
    "    \"\"\"\n",
    "    Forward propagation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_sequences, n_features)\n",
    "        Input\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    ndarray, shape (batch_size, n_nodes)\n",
    "        State/Output\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.dot(x, w_x) + np.dot(h, w_h) + b\n",
    "    \n",
    "    tanh = Tanh()\n",
    "    h = tanh.forward(a)\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76188798 0.76213958 0.76239095 0.76255841]]\n",
      "[[0.792209   0.8141834  0.83404912 0.84977719]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# Forward propagtion\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    h = forward(x[0][i], w_x, w_h, h, b)\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "[[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "print(h)\n",
    "print(correct_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] Fit and Predict Dataset\n",
    "\n",
    "<br />\n",
    "\n",
    "\"IMDB Review Dataset\" on Kaggle\n",
    "\n",
    "\n",
    "https://www.kaggle.com/utathya/imdb-review-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "from imp import reload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 1933, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2fe8baa90248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb_master.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/envs/dive/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/envs/dive/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/envs/dive/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.0/envs/dive/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 1933, saw 2\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"imdb_master.csv\", delimiter=\"\\t\")\n",
    "df1 = df1.drop(['id'], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_epoch : int\n",
    "        Number of epochs\n",
    "    \n",
    "    batch_size : int\n",
    "        Size of batch\n",
    "    \n",
    "    verbose : bool\n",
    "        True if outputting learning process\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    loss : list\n",
    "        List of arrays of records of loss on train dataset\n",
    "    \n",
    "    val_loss : list\n",
    "        List of arrays of records of loss on validation dataset\n",
    "    \n",
    "    layers : list\n",
    "        List of layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_epoch, batch_size, verbose=True):\n",
    "        self.epoch = num_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.layers = []\n",
    "    \n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers += [layer]\n",
    "    \n",
    "    \n",
    "    def forward_layer(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def backward_layer(self, y):\n",
    "        for layer in reversed(self.layers):\n",
    "            y = layer.backward(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_sequences, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y : ndarray, shape (n_sequences, )\n",
    "            Correct values of train dataset\n",
    "        \n",
    "        X_val : ndarray, shape (n_sequences, n_features)\n",
    "            Features of validation dataset\n",
    "        \n",
    "        y_val : ndarray, shape (n_sequences, )\n",
    "            Correct values of validation dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            count = 0\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            # Mini-Batch Processing\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                get_mini_batch_val = GetMiniBatch(X_val, y_val, batch_size=self.batch_size)\n",
    "                # Loop per iteration\n",
    "                for ((mini_X_train, mini_y_train), (mini_X_val_train, mini_y_val_train)) in zip(get_mini_batch, \n",
    "                                                                                                get_mini_batch_val):\n",
    "                    # Forward propagation\n",
    "                    Z = self.forward_layer(mini_X_train)\n",
    "                    Z_val = self.forward_layer(mini_X_val_train)\n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        loss = Loss()\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z)\n",
    "                        L_val = loss.cross_entropy_loss(mini_y_val_train, Z_val)\n",
    "                    # Backforward propagation\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "                    dX_val = self.backward_layer(mini_y_val_train)\n",
    "            else:\n",
    "                # Loop per iteration\n",
    "                for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                    # Forward propagation\n",
    "                    Z = self.forward_layer(mini_X_train)\n",
    "                    # Loss\n",
    "                    if self.verbose:\n",
    "                        loss = Loss()\n",
    "                        L = loss.cross_entropy_loss(mini_y_train, Z)\n",
    "                    # Backforward propagation\n",
    "                    dX = self.backward_layer(mini_y_train)\n",
    "            \n",
    "            # Output learning process\n",
    "            if self.verbose:\n",
    "                self.loss += [sum(L) / self.batch_size]\n",
    "                if (X_val is not None) and (y_val is not None):\n",
    "                    self.val_loss += [sum(L_val) / self.batch_size]\n",
    "                    print(\"{0}ep loss: {1}, val_loss: {2}\".format(count+1, self.loss[count], self.val_loss[count]))\n",
    "                else:\n",
    "                    print(self.loss[count])\n",
    "                count += 1\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_sequences, n_features)\n",
    "            Samples\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray, shape (n_sequences, 1)\n",
    "            Results of prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        Z = self.forward_layer(X)\n",
    "        \n",
    "        return np.argmax(Z, axis=1)\n",
    "    \n",
    "    \n",
    "    def plot_learning_record(self):\n",
    "        plt.figure(facecolor=\"azure\", edgecolor=\"coral\")\n",
    "        plt.plot(self.loss, label=\"loss\")\n",
    "        plt.plot(self.val_loss, label=\"val_loss\")\n",
    "        plt.title(\"Learning Records\")\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    def compute_index_values(self, y, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray, shape(n_sequences, n_features)\n",
    "            Features of train dataset\n",
    "        \n",
    "        y: ndarray, shape(n_sequences,)\n",
    "            Correct values of train dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"accuracy score:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct\n",
    "\n",
    "nn = NeuralNetwork(100, 10)\n",
    "nn.add(SimpleRNN(batch_size, n_sequences, n_features, n_nodes, SimpleInitializer(sigma=0.01), SGD_RNN(lr=0.001)))\n",
    "nn.add(FC(784, 400, SimpleInitializer(sigma=0.01), SGD_FC(lr=0.001)))\n",
    "nn.add(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "\n",
    "dnn.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
